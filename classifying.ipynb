{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuyuan/.local/lib/python3.8/site-packages/geopandas/_compat.py:124: UserWarning: The Shapely GEOS version (3.11.2-CAPI-1.17.2) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_28810/2891674836.py:4: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import re\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification of real estate brokerages\n",
    "\n",
    "We first classify the real estate brokerages of the whole country into several main parts by the first four names, and then we extract 200 main real estate brokerages in the whole country and extract the first two words in the result. Then after extracting, we give label to each of the dataset and remerging the dataset to calculate the HHI index and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all the csv files in the directory\n",
    "for i in range(16, 23):\n",
    "    folder_path = '/home/xuyuan/Desktop/RealEstateBrokerage/classifying brokerages/whole dataset/whole/{}/'.format(i)\n",
    "    csv_files = glob.glob(folder_path + '/**/*.csv', recursive=True)\n",
    "\n",
    "    dfs = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(folder_path, file))\n",
    "        # print(file)\n",
    "        dfs = pd.concat([dfs, df], ignore_index=True)\n",
    "    dfs.to_csv('/home/xuyuan/Desktop/RealEstateBrokerage/classifying brokerages/combined_result/{}_whole.csv'.format(i), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all the csv files in the directory\n",
    "for i in range(16, 23):\n",
    "    folder_path = '/home/xuyuan/Desktop/RealEstateBrokerage/classifying brokerages/whole dataset/lianjia_beke/20{} cleaned/'.format(i)\n",
    "    csv_files = glob.glob(folder_path + '/**/*.csv', recursive=True)\n",
    "    \n",
    "    dfs = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(folder_path, file))\n",
    "        # print(file)\n",
    "        dfs = pd.concat([dfs, df], ignore_index=True)\n",
    "    dfs.to_csv('/home/xuyuan/Desktop/RealEstateBrokerage/classifying brokerages/combined_result/{}_lianjia.csv'.format(i), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extracting the first two words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now I need to merge our dataset with the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chinese(text):\n",
    "    return ''.join(re.findall(r'[\\u4e00-\\u9fff]+', text))\n",
    "\n",
    "def extract_number(text):\n",
    "    return ''.join(re.findall(r'[0-9]+', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(16, 23):\n",
    "    folder_path = '/home/xuyuan/Desktop/RealEstateBrokerage/classifying brokerages/broker/'\n",
    "    csv_files = glob.glob(folder_path + '/**/*.csv', recursive=True)\n",
    "\n",
    "    dfs = pd.DataFrame()\n",
    "    for file in csv_files:\n",
    "         \n",
    "        df = pd.read_csv(os.path.join(folder_path, file))\n",
    "        df['year'] = extract_number(file)\n",
    "        df['Chinese_Name'] = extract_chinese(file)\n",
    "        df = df[~df['name'].str.contains('职业介绍|黄马褂|临时工|停车场|便利店|口腔诊所|自助银行|鲜花|数码冲印')]\n",
    "        df.loc[df['name'].str.contains(r'链家|贝壳|Lianjia|lianjia|壳点|自如', na=False), 'name'] = '链家'\n",
    "        # print(file)\n",
    "        dfs = pd.concat([dfs, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505672\n"
     ]
    }
   ],
   "source": [
    "print(len(dfs))\n",
    "# 506353\n",
    "# 505672"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['16', '22', '21', '20', '19', '18', '17'], dtype=object)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.to_csv('/home/xuyuan/Desktop/RealEstateBrokerage/classifying brokerages/combined_result/16_22_broker_10cities.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/xuyuan/Desktop/RealEstateBrokerage/classifying brokerages/combined_result/16_22_broker_10cities.csv')\n",
    "df = pd.read_csv('/home/xuyuan/Desktop/RealEstateBrokerage/classifying brokerages/combined_result/check.csv')\n",
    "data['first_two'] = data['name'].str[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(data, df, left_on='first_two', right_on='two', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('/home/xuyuan/Desktop/RealEstateBrokerage/classifying brokerages/combined_result/16_22_broker_10cities_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/xuyuan/Desktop/RealEstateBrokerage/classifying brokerages/combined_result/16_22_broker_10cities_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chinese_names = data['Chinese_Name'].unique()\n",
    "unique_years = data['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16, 22, 21, 20, 19, 18, 17])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chinese_name in unique_chinese_names:\n",
    "    for year in unique_years:\n",
    "        # Create a subset based on the current Chinese name and year\n",
    "        subset = data[(data['Chinese_Name'] == chinese_name) & (data['year'] == year)]\n",
    "\n",
    "        file_name = f'{year}_{chinese_name}.csv'\n",
    "        subset.to_csv(os.path.join('/home/xuyuan/Desktop/RealEstateBrokerage/classifying brokerages/cleaned', file_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now final clean the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../cleaned_district_Jan_4.csv')\n",
    "dataframes_list = []\n",
    "for i in data['chinese_csv'].unique():\n",
    "    dataframes_list.append(data[data['chinese_csv'] == i])\n",
    "\n",
    "for i in range(len(dataframes_list)):\n",
    "    dataframes_list[i] = gpd.GeoDataFrame(dataframes_list[i], geometry=dataframes_list[i]['geometry'].apply(wkt.loads))\n",
    "    dataframes_list[i] = dataframes_list[i].set_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_chinese_names = data['chinese_csv'].unique()\n",
    "chinese_name_to_index = {name: i for i, name in enumerate(unique_chinese_names)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_name_to_index['北京市17.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/xuyuan/Desktop/RealEstateBrokerage/classifying brokerages/cleaned'\n",
    "csv_files = glob.glob(folder_path + '/**/*.csv', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_csv_files = []\n",
    "\n",
    "for file in csv_files:\n",
    "    city_name = extract_chinese(file)\n",
    "    number = extract_number(file)\n",
    "    file_name = '{}{}.csv'.format(city_name, number)\n",
    "\n",
    "    # Check if file_name is in the dictionary\n",
    "    if file_name in chinese_name_to_index:\n",
    "        valid_csv_files.append(file)  # Add to the valid list if the condition is met\n",
    "\n",
    "csv_files = valid_csv_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for file in csv_files:\n",
    "    city_name = extract_chinese(file)\n",
    "    number = extract_number(file)\n",
    "    file_name = '{}{}.csv'.format(city_name, number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import find_stores_within\n",
    "\n",
    "for file in csv_files:\n",
    "    city_name = extract_chinese(file)\n",
    "    number = extract_number(file)\n",
    "    file_name = '{}{}.csv'.format(city_name, number)\n",
    "    i = chinese_name_to_index[file_name]\n",
    "        \n",
    "    df = pd.read_csv(os.path.join(folder_path, file))\n",
    "\n",
    "    store_locations = gpd.GeoDataFrame(df, geometry = gpd.points_from_xy(df['gpsx'], df['gpsy']))    \n",
    "    store_locations['label'].fillna(-1, inplace = True)\n",
    "    store_locations_cpp = [list(point.coords[0]) for point in store_locations['geometry']]\n",
    "        \n",
    "    communities_gdf = dataframes_list[i]\n",
    "    community_locations_cpp = [list(point.coords[0]) for point in communities_gdf['geometry']]\n",
    "    \n",
    "    \n",
    "    labels = [int(label) for label in store_locations['label'].tolist()]\n",
    "    threshold_distance = 0.42 # 420 meters\n",
    "    result = find_stores_within.find_stores_within(store_locations_cpp, \n",
    "                                               community_locations_cpp, \n",
    "                                               labels,\n",
    "                                               threshold_distance)\n",
    "    \n",
    "    dataframes_list[i]['label'] = result[0]\n",
    "    \n",
    "    # we first calculate for each row, how many lianjia are within the distance of the community.\n",
    "    dataframes_list[i]['lianjia_420'] = dataframes_list[i]['label'].apply(lambda x: x.count(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now we use the label to construct the HHI index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  [1, 4, 0, 0]\n",
       "1                   [0, -1, -1]\n",
       "2       [0, 0, 1, 27, -1, 1, 1]\n",
       "3                [28, 4, 0, 28]\n",
       "4                           [0]\n",
       "                 ...           \n",
       "5434                         []\n",
       "5435             [0, 28, 1, 28]\n",
       "5436                       [-1]\n",
       "5437                    [1, -1]\n",
       "5438                [0, -1, -1]\n",
       "Name: label, Length: 5439, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframes_list[0]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g++ -O3 -shared -std=gnu++17 -fPIC -I/usr/include/python3.8 -I/home/xuyuan/.local/lib/python3.8/site-packages/pybind11/include -o hhi_calculator.so hhi_calculator.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hhi_calculator \n",
    "\n",
    "for i in range(0, len(dataframes_list)):\n",
    "    data = dataframes_list[i]['label'].tolist()\n",
    "    \n",
    "    # you also need to clculate the number of elements in the list\n",
    "    dataframes_list[i]['broker_420'] = [len(sublist) for sublist in data]\n",
    "    hhi_indices = hhi_calculator.calculate_HHI(data)\n",
    "    dataframes_list[i]['hhi'] = hhi_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.DataFrame()\n",
    "for i in range(0, len(dataframes_list)):\n",
    "    dataframes_list[i]['hhi'].fillna(0, inplace=True)\n",
    "    final_data = pd.concat([final_data, dataframes_list[i]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222837\n"
     ]
    }
   ],
   "source": [
    "print(len(final_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('../cleaned_district_Jan_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
