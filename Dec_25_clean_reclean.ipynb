{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuyuan/.local/lib/python3.8/site-packages/geopandas/_compat.py:124: UserWarning: The Shapely GEOS version (3.11.2-CAPI-1.17.2) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_24335/3939382229.py:4: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the code book of the data\n",
    "\n",
    "- 'building_type': The classification of a particular building.\n",
    "- 'village': The region that it belongs to.\n",
    "- 'district': administrative division that the Community belongs to.\n",
    "- 'floor_level': The level on which a particular room or apartment is, within a building.\n",
    "- 'new_lng': the longitude coordinates.\n",
    "- 'new_lat': the latitude coordinates.\n",
    "- 'year': time id.\n",
    "- 'floor_ratio': The ratio of the floor area to the total plot area.\n",
    "- 'green_ratio': The ratio of the green space to the total plot area.\n",
    "- 'nego_times': The number of times a negotiation was held.\n",
    "- 'lead_times': The time it takes before a deal is made.\n",
    "- 'total_building': The total number of buildings in an area.\n",
    "- 'total_resident': The total number of residents in an area.\n",
    "- 'watching_people': The number of people watching a listing.\n",
    "- 'watched_times': The number of times a listing is watched.\n",
    "- 'striker_price': The initial asking price.\n",
    "- 'striker_price_pers': The asking price per square foot.\n",
    "- 'end_price': The final agreed price.\n",
    "- 'end_price_pers': The final agreed price per square foot.\n",
    "- 'area': The area of a property.\n",
    "- 'nego_period': The period over which negotiations took place.\n",
    "- 'bedroom': The number of bedrooms in a property.\n",
    "- 'living_room': The number of living rooms in a property.\n",
    "- 'kitchen': The number of kitchens in a property.\n",
    "- 'toilet': The number of toilets in a property.\n",
    "- 'total_floor_number': The number of floors in a building.\n",
    "- 'elevator_ratio': The ratio of elevators to the total number of floors.\n",
    "- 'house_age': The age of the house.\n",
    "- 'income': The income lianjia in this given district.\n",
    "- 'number': The number lianjia in this given district.\n",
    "- 'super': referring to proximity to supermarkets (measured by number within given distance).\n",
    "- 'sub': referring to proximity to subway stations.\n",
    "- 'hotel': referring to proximity to hotels\n",
    "- 'kind': referring to proximity to kindergartens\n",
    "- 'prim': referring to primary schools.\n",
    "- 'mid': referring to middle schools.\n",
    "- 'shop_mall': referring to shopping mall.\n",
    "- 'west_food': referring to the availability of western food nearby.\n",
    "- 'park': referring to parks.\n",
    "- 'museum': Distance to the nearest museum.\n",
    "- 'ktv': referring to KTV and some entertainment venues.\n",
    "- 'jiadian': referring to electronic shops.\n",
    "- 'old': referring to old care systems.\n",
    "- 'other': other real estate brokerages within 1km.\n",
    "- 'other_5': other real estate brokerages within 0.5km.\n",
    "- 'lianjia': lianjia's number within 1km.\n",
    "- 'lianjia_5': lianjia's number within 0.5km.\n",
    "- 'beke': beke's number within 1km.\n",
    "- 'beke_5': beke's number within 0.5km.\n",
    "- 'geometry': geometry information.\n",
    "- 'light': night time lights.\n",
    "- 'pop': population density.\n",
    "- 'pm25': Air quality measure.\n",
    "- 'region': city name.\n",
    "- 'id': unique id.\n",
    "- 'business_area': business area.\n",
    "- 'index_right': unique index id \n",
    "- 'num': transaction number within 1km\n",
    "- 'prft': lianjia's income within 1km\n",
    "- 'price' housing price within 1km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we extract the number of transactions within the given 1km geometry to map with the community level data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the below codes are for merging the 1km data to the district level data\n",
    "\n",
    "you do not need to execute them because I have already merged it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuyuan/.local/lib/python3.8/site-packages/geopandas/_compat.py:124: UserWarning: The Shapely GEOS version (3.11.2-CAPI-1.17.2) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_21822/1062130048.py:1: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('cleaned_1km.csv')\n",
    "df = pd.read_csv('cleaned_district.csv')\n",
    "data.drop(columns = 'index', inplace=True)\n",
    "\n",
    "# Create GeoDataFrames\n",
    "df_copy = gpd.GeoDataFrame(df.copy(), geometry=df['geometry'].apply(wkt.loads))\n",
    "data_copy = gpd.GeoDataFrame(data.copy(), geometry=data['geometry'].apply(wkt.loads))\n",
    "des = gpd.GeoDataFrame(data.drop_duplicates(subset=['geometry'], keep='first').copy(), \n",
    "                       geometry=data.drop_duplicates(subset=['geometry'], keep='first')['geometry'].apply(wkt.loads))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df_copy.set_crs('epsg:4326')\n",
    "data_copy = data_copy.set_crs('epsg:4326')\n",
    "des = des.set_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy['price'] = data_copy['price'] / data_copy['area']\n",
    "# this would guarantee that the result is the average price per square meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuyuan/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "if 'index_left' in df_copy.columns:\n",
    "    df_copy.drop(columns=['index_left'], inplace=True)\n",
    "if 'index_right' in df_copy.columns:\n",
    "    df_copy.drop(columns=['index_right'], inplace=True)\n",
    "\n",
    "# Check and rename/drop 'index_left' and 'index_right' in des\n",
    "if 'index_left' in des.columns:\n",
    "    des.drop(columns=['index_left'], inplace=True)\n",
    "if 'index_right' in des.columns:\n",
    "    des.drop(columns=['index_right'], inplace=True)\n",
    "    \n",
    "joined_gdf = gpd.sjoin(df_copy, des, how=\"left\", op='within')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined_gdf['id_unique', 'year'] # id_unique is the unique identification of the data object\n",
    "# and now we shall join the data to the district data\n",
    "\n",
    "data_copy['unique_key'] = data_copy['id_unique'].astype(str) + '_' + data_copy['year'].astype(str)\n",
    "joined_gdf['unique_key'] = joined_gdf['id_unique'].astype(str) + '_' + joined_gdf['year_left'].astype(str)\n",
    "data_relevant = data_copy[['unique_key', 'prft', 'num', 'price']]\n",
    "\n",
    "joined_gdf = joined_gdf.merge(data_relevant, on='unique_key', how='left')\n",
    "joined_gdf.drop(columns=['unique_key'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['unique_key'] = df_copy['id'].astype(str) + '_' + df_copy['year'].astype(str)\n",
    "joined_gdf['unique_key'] = joined_gdf['id_left'].astype(str) + '_' + joined_gdf['year_left'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf.rename(columns = {'prft': 'region_income', 'num': 'region_num', 'price': 'region_price'}, inplace=True)\n",
    "data_relevant = joined_gdf[['unique_key', 'region_income', 'region_num', 'region_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df_copy.merge(data_relevant, on='unique_key', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.drop(columns=['unique_key', 'num', 'prft', 'price'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.to_csv('cleaned_district_Jan.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_district_Jan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by = ['id', 'year'], inplace = True)\n",
    "data['lag_lianjia'] = data.groupby('id')['lianjia_5'].shift(1)\n",
    "data['lag_lianjia'] = data['lag_lianjia'].fillna(data['lianjia_5'])\n",
    "data[['id', 'lianjia_5', 'lag_lianjia']]\n",
    "data['entry'] = (data['lianjia_5'] > data['lag_lianjia']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "for i in range(1, n + 1):\n",
    "    data[f'post{i}'] = data.groupby('id')['entry'].shift(i).fillna(0)\n",
    "for i in range(1, n + 1):\n",
    "    data[f'pre{i}'] = data.groupby('id')['entry'].shift(-i).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('cleaned_district_Jan_2.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now we construct the RD design dataset\n",
    "\n",
    "the RD design dataset contains the following properties:\n",
    "\n",
    "First, we extract a list of csv files located in the lianjia_beke directory within the given working path. Then we map these files to the design of communities and extract each lianjia store with its nearest community or nearest two communities respectively. Then we conduct the RD analysis in the stata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_district_Jan_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the directory path\n",
    "directory_path = \"lianjia_beke\"\n",
    "filenames = []\n",
    "# List all files and directories in the given path\n",
    "for filename in os.listdir(directory_path):\n",
    "    filenames.append(filename)\n",
    "# data['region'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_name_mapping = {\n",
    "    'beijing': '北京市',\n",
    "    'chengdu': '成都市',\n",
    "    'chongqing': '重庆市',\n",
    "    'guangzhou': '广州市',\n",
    "    'hangzhou': '杭州市',\n",
    "    'nanjing': '南京市',\n",
    "    'shanghai': '上海市',\n",
    "    'shenzhen': '深圳市',\n",
    "    'tianjin': '天津市',\n",
    "    'wuhan': '武汉市',\n",
    "    'xian': '西安市'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_chinese_csv(row):\n",
    "    year_suffix = str(row['year'])[-2:]  # Extract the last two digits of the year\n",
    "    chinese_city = city_name_mapping[row['region']]  # Map to Chinese city name\n",
    "    return f\"{chinese_city}{year_suffix}.csv\"  # Combine to form the Chinese CSV file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['chinese_csv'] = data.apply(map_to_chinese_csv, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list = []\n",
    "for i in data['chinese_csv'].unique():\n",
    "    dataframes_list.append(data[data['chinese_csv'] == i])\n",
    "\n",
    "for i in range(len(dataframes_list)):\n",
    "    dataframes_list[i] = gpd.GeoDataFrame(dataframes_list[i], geometry=dataframes_list[i]['geometry'].apply(wkt.loads))\n",
    "    dataframes_list[i] = dataframes_list[i].set_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "\n",
    "the code below extract the number of nearest community for each lianjia's store. It used nested for loop to achieve this and this code runs pretty long time, we can optimize it using Cpp.\n",
    "\n",
    "For computing purpose, I suppose to use the Cpp codes in the second block\n",
    "\n",
    "procedure to build it (you should revise the location in your computer):\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "pip install pybind11\n",
    "\n",
    "python3-config --cflags\n",
    "\n",
    "pybind11-config --includes\n",
    "\n",
    "cd the/file/path/RealEstateBrokerage/\n",
    "\n",
    "g++ -O3 -shared -std=c++11 -fPIC -I/usr/include/python3.8 -I/home/xuyuan/.local/lib/python3.8/site-packages/pybind11/include -o nearest_community_cpp.so nearest_community_cpp.cpp\n",
    "```\n",
    "\n",
    "the running time is less than 10 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for i in range(0, len(dataframes_list)):\n",
    "    communities_gdf = dataframes_list[i]\n",
    "    df = pd.read_csv('lianjia_beke/' + communities_gdf['chinese_csv'].unique()[0])\n",
    "    store_locations_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['gpsx'], df['gpsy']))\n",
    "    # store_locations_gdf['nearest_community_index'] = -1\n",
    "    \n",
    "    num_nearest_communities = 2\n",
    "    \n",
    "    nearest_community_indices_list = []\n",
    "    \n",
    "    for store_index, store_row in store_locations_gdf.iterrows():\n",
    "        store_location = store_row['geometry']\n",
    "        \n",
    "        # Calculate distances to all communities and store them in a Series\n",
    "        distances = communities_gdf.geometry.apply(lambda x: store_location.distance(x))\n",
    "        \n",
    "        # Sort the distances and select the indices of the nearest communities\n",
    "        nearest_community_indices = distances.argsort()[:num_nearest_communities].tolist()\n",
    "        \n",
    "        # Append the list of nearest community indices to the list\n",
    "        nearest_community_indices_list.append(nearest_community_indices)\n",
    "        \n",
    "    # Assign the list to the 'nearest_community_indices' column\n",
    "    store_locations_gdf['nearest_community_indices'] = nearest_community_indices_list\n",
    "    \n",
    "    store_locations_gdf.to_csv('nearest_community/' + communities_gdf['chinese_csv'].unique()[0], index = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nearest_community_cpp\n",
    "directory_path = \"nearest_community\"\n",
    "if not os.path.exists(directory_path):\n",
    "    # If it doesn't exist, create the directory\n",
    "    os.makedirs(directory_path)\n",
    "\n",
    "for i in range(0, len(dataframes_list)):\n",
    "    communities_gdf = dataframes_list[i]\n",
    "    community_locations_cpp = [list(point.coords[0]) for point in communities_gdf['geometry']]\n",
    "    df = pd.read_csv('lianjia_beke/' + communities_gdf['chinese_csv'].unique()[0])\n",
    "    store_locations_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['gpsx'], df['gpsy']))\n",
    "    store_locations_cpp = [list(point.coords[0]) for point in store_locations_gdf['geometry']]\n",
    "    \n",
    "    \n",
    "    num_nearest_communities = 2 # you may change this number whatever you want\n",
    "    nearest_community_indices_list = \\\n",
    "        nearest_community_cpp.find_nearest_communities(store_locations_cpp, community_locations_cpp, num_nearest_communities)\n",
    "    \n",
    "    # Assign the list to the 'nearest_community_indices' column\n",
    "    store_locations_gdf['nearest_community_indices'] = nearest_community_indices_list\n",
    "    \n",
    "    store_locations_gdf.to_csv('nearest_community/' + communities_gdf['chinese_csv'].unique()[0], index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
