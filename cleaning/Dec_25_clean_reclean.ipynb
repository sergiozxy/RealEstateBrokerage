{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuyuan/.local/lib/python3.8/site-packages/geopandas/_compat.py:124: UserWarning: The Shapely GEOS version (3.11.2-CAPI-1.17.2) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_5454/3318037310.py:4: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "import re\n",
    "\n",
    "os.chdir('../') # set working directory to root of repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we extract the number of transactions within the given 1km geometry to map with the community level data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the below codes are for merging the 1km data to the district level data\n",
    "\n",
    "you do not need to execute them because I have already merged it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_1km.csv')\n",
    "df = pd.read_csv('uncleaned_district.csv')\n",
    "data.rename(columns={'prft':'district_income', 'num': 'district_num', 'price': 'district_price'}, inplace=True)\n",
    "data.drop(columns = 'index', inplace=True)\n",
    "\n",
    "# Create GeoDataFrames\n",
    "df_copy = gpd.GeoDataFrame(df.copy(), geometry=df['geometry'].apply(wkt.loads))\n",
    "data_copy = gpd.GeoDataFrame(data.copy(), geometry=data['geometry'].apply(wkt.loads))\n",
    "des = gpd.GeoDataFrame(data.drop_duplicates(subset=['geometry'], keep='first').copy(), \n",
    "                       geometry=data.drop_duplicates(subset=['geometry'], keep='first')['geometry'].apply(wkt.loads))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'year', 'top', 'left', 'id', 'right', 'bottom', 'ngtm',\n",
       "       'living', 'flnum', 'wtpp', 'tihu', 'pripe', 'mall', 'ldtm', 'numbu',\n",
       "       'diff', 'west', 'kind', 'hotel', 'muse', 'super', 'nego', 'jd',\n",
       "       'grrate', 'prim', 'broke', 'dfrat', 'district_income', 'flrate',\n",
       "       'reside', 'district_num', 'wttm', 'toilet', 'ktv', 'bedroo', 'peri',\n",
       "       'park', 'lttm', 'mid', 'district_price', 'kitche', 'old', 'area',\n",
       "       'fllvl', 'age', 'sub', 'lianj', 'light', 'pop', 'pm25', 'geometry',\n",
       "       'city', 'id_unique', 'point_value', 'grppr', 'grpnu', 'grpic'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['building_type', 'village', 'district', 'floor_level', 'new_lng',\n",
       "       'new_lat', 'year', 'floor_ratio', 'green_ratio', 'nego_times',\n",
       "       'lead_times', 'total_building', 'total_resident', 'watching_people',\n",
       "       'watched_times', 'striker_price', 'striker_price_pers', 'end_price',\n",
       "       'end_price_pers', 'area', 'nego_period', 'bedroom', 'living_room',\n",
       "       'kitchen', 'toilet', 'total_floor_number', 'elevator_ratio',\n",
       "       'house_age', 'income', 'number', 'super', 'sub', 'hotel', 'kind',\n",
       "       'prim', 'mid', 'shop_mall', 'west_food', 'park', 'museum', 'ktv',\n",
       "       'jiadian', 'old', 'other', 'other_5', 'lianjia', 'lianjia_5', 'beke',\n",
       "       'beke_5', 'geometry', 'light', 'pop', 'pm25', 'region', 'id',\n",
       "       'business_area'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df_copy.set_crs('epsg:4326')\n",
    "data_copy = data_copy.set_crs('epsg:4326')\n",
    "des = des.set_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy['district_price'] = data_copy['district_price'] / data_copy['area']\n",
    "# this would guarantee that the result is the average price per square meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuyuan/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "if 'index_left' in df_copy.columns:\n",
    "    df_copy.drop(columns=['index_left'], inplace=True)\n",
    "if 'index_right' in df_copy.columns:\n",
    "    df_copy.drop(columns=['index_right'], inplace=True)\n",
    "\n",
    "# Check and rename/drop 'index_left' and 'index_right' in des\n",
    "if 'index_left' in des.columns:\n",
    "    des.drop(columns=['index_left'], inplace=True)\n",
    "if 'index_right' in des.columns:\n",
    "    des.drop(columns=['index_right'], inplace=True)\n",
    "    \n",
    "joined_gdf = gpd.sjoin(df_copy, des, how=\"left\", op='within')\n",
    "joined_gdf.drop(columns = ['district_income', 'district_num', 'district_price'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined_gdf['id_unique', 'year'] # id_unique is the unique identification of the data object\n",
    "# and now we shall join the data to the district data\n",
    "\n",
    "data_copy['unique_key'] = data_copy['id_unique'].astype(str) + '_' + data_copy['year'].astype(str)\n",
    "joined_gdf['unique_key'] = joined_gdf['id_unique'].astype(str) + '_' + joined_gdf['year_left'].astype(str)\n",
    "data_relevant = data_copy[['unique_key', 'district_income', 'district_num', 'district_price']]\n",
    "\n",
    "joined_gdf = joined_gdf.merge(data_relevant, on='unique_key', how='left')\n",
    "joined_gdf.drop(columns=['unique_key'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['unique_key'] = df_copy['id'].astype(str) + '_' + df_copy['year'].astype(str)\n",
    "joined_gdf['unique_key'] = joined_gdf['id_left'].astype(str) + '_' + joined_gdf['year_left'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf.rename(columns = {'district_income': 'region_income', 'district_num': 'region_num', 'district_price': 'region_price'}, inplace=True)\n",
    "data_relevant = joined_gdf[['unique_key', 'region_income', 'region_num', 'region_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df_copy.merge(data_relevant, on='unique_key', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.to_csv('cleaned_district_Jan.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_district_Jan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by = ['id', 'year'], inplace = True)\n",
    "data['lag_lianjia'] = data.groupby('id')['lianjia_5'].shift(1)\n",
    "data['lag_lianjia'] = data['lag_lianjia'].fillna(data['lianjia_5'])\n",
    "data[['id', 'lianjia_5', 'lag_lianjia']]\n",
    "data['entry'] = (data['lianjia_5'] > data['lag_lianjia']).astype(int)\n",
    "\n",
    "### note that this function is wrong!!!!!!!!\n",
    "# and we should just drop it right here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "for i in range(1, n + 1):\n",
    "    data[f'post{i}'] = data.groupby('id')['entry'].shift(i).fillna(0)\n",
    "for i in range(1, n + 1):\n",
    "    data[f'pre{i}'] = data.groupby('id')['entry'].shift(-i).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('cleaned_district_Jan_2.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now we construct the RD design dataset\n",
    "\n",
    "the RD design dataset contains the following properties:\n",
    "\n",
    "First, we extract a list of csv files located in the lianjia_beke directory within the given working path. Then we map these files to the design of communities and extract each lianjia store with its nearest community or nearest two communities respectively. Then we conduct the RD analysis in the stata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_district_Jan_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the directory path\n",
    "directory_path = \"lianjia_beke\"\n",
    "filenames = []\n",
    "# List all files and directories in the given path\n",
    "for filename in os.listdir(directory_path):\n",
    "    filenames.append(filename)\n",
    "# data['region'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_name_mapping = {\n",
    "    'beijing': '北京市',\n",
    "    'chengdu': '成都市',\n",
    "    'chongqing': '重庆市',\n",
    "    'guangzhou': '广州市',\n",
    "    'hangzhou': '杭州市',\n",
    "    'nanjing': '南京市',\n",
    "    'shanghai': '上海市',\n",
    "    'shenzhen': '深圳市',\n",
    "    'tianjin': '天津市',\n",
    "    'wuhan': '武汉市',\n",
    "    'xian': '西安市'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_chinese_csv(row):\n",
    "    year_suffix = str(row['year'])[-2:]  # Extract the last two digits of the year\n",
    "    chinese_city = city_name_mapping[row['region']]  # Map to Chinese city name\n",
    "    return f\"{chinese_city}{year_suffix}.csv\"  # Combine to form the Chinese CSV file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['chinese_csv'] = data.apply(map_to_chinese_csv, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list = []\n",
    "for i in data['chinese_csv'].unique():\n",
    "    dataframes_list.append(data[data['chinese_csv'] == i])\n",
    "\n",
    "for i in range(len(dataframes_list)):\n",
    "    dataframes_list[i] = gpd.GeoDataFrame(dataframes_list[i], geometry=dataframes_list[i]['geometry'].apply(wkt.loads))\n",
    "    dataframes_list[i] = dataframes_list[i].set_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "\n",
    "the code below extract the number of nearest community for each lianjia's store. It used nested for loop to achieve this and this code runs pretty long time, we can optimize it using Cpp.\n",
    "\n",
    "For computing purpose, I suppose to use the Cpp codes in the second block\n",
    "\n",
    "procedure to build it (you should revise the location in your computer):\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "pip install pybind11\n",
    "\n",
    "python3-config --cflags\n",
    "\n",
    "pybind11-config --includes\n",
    "\n",
    "cd the/file/path/RealEstateBrokerage/\n",
    "\n",
    "g++ -O3 -shared -std=c++11 -fPIC -I/usr/include/python3.8 -I/home/xuyuan/.local/lib/python3.8/site-packages/pybind11/include -o nearest_community_cpp.so nearest_community_cpp.cpp\n",
    "```\n",
    "\n",
    "the running time is less than 25 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for i in range(0, len(dataframes_list)):\n",
    "    communities_gdf = dataframes_list[i]\n",
    "    df = pd.read_csv('lianjia_beke/' + communities_gdf['chinese_csv'].unique()[0])\n",
    "    store_locations_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['gpsx'], df['gpsy']))\n",
    "    # store_locations_gdf['nearest_community_index'] = -1\n",
    "    \n",
    "    num_nearest_communities = 2\n",
    "    \n",
    "    nearest_community_indices_list = []\n",
    "    \n",
    "    for store_index, store_row in store_locations_gdf.iterrows():\n",
    "        store_location = store_row['geometry']\n",
    "        \n",
    "        # Calculate distances to all communities and store them in a Series\n",
    "        distances = communities_gdf.geometry.apply(lambda x: store_location.distance(x))\n",
    "        \n",
    "        # Sort the distances and select the indices of the nearest communities\n",
    "        nearest_community_indices = distances.argsort()[:num_nearest_communities].tolist()\n",
    "        \n",
    "        # Append the list of nearest community indices to the list\n",
    "        nearest_community_indices_list.append(nearest_community_indices)\n",
    "        \n",
    "    # Assign the list to the 'nearest_community_indices' column\n",
    "    store_locations_gdf['nearest_community_indices'] = nearest_community_indices_list\n",
    "    \n",
    "    store_locations_gdf.to_csv('nearest_community/' + communities_gdf['chinese_csv'].unique()[0], index = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nearest_community_cpp\n",
    "\n",
    "directory_path = \"nearest_community\"\n",
    "if not os.path.exists(directory_path):\n",
    "    # If it doesn't exist, create the directory\n",
    "    os.makedirs(directory_path)\n",
    "\n",
    "for i in range(0, len(dataframes_list)):\n",
    "    communities_gdf = dataframes_list[i]\n",
    "    community_locations_cpp = [list(point.coords[0]) for point in communities_gdf['geometry']]\n",
    "    df = pd.read_csv('lianjia_beke/' + communities_gdf['chinese_csv'].unique()[0])\n",
    "    store_locations_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['gpsx'], df['gpsy']))\n",
    "    store_locations_cpp = [list(point.coords[0]) for point in store_locations_gdf['geometry']]\n",
    "    \n",
    "    num_nearest_communities = 2 # you may change this number whatever you want\n",
    "    nearest_community_indices_tuple = \\\n",
    "        nearest_community_cpp.find_nearest_communities(store_locations_cpp, community_locations_cpp, num_nearest_communities)\n",
    "    \n",
    "    nearest_community_indices_list, nearest_community_distances_list = nearest_community_indices_tuple\n",
    "\n",
    "    # Assign the list to the 'nearest_community_indices' column\n",
    "    store_locations_gdf['nearest_community_indices'] = nearest_community_indices_list\n",
    "    store_locations_gdf['nearest_community_distances'] = nearest_community_distances_list\n",
    "    \n",
    "    store_locations_gdf.to_csv('nearest_community/' + communities_gdf['chinese_csv'].unique()[0], index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we merge it back to our dataframe to get the original dataframe. Note that the order gonna be:\n",
    "\n",
    "1. The index of the nearest community to the store location.\n",
    "2. The index of the second nearest community to the store location.\n",
    "3. If there are more communities to consider (i.e., num_nearest_communities is greater than 2), the indices of the subsequent nearest communities will follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>gpsx</th>\n",
       "      <th>gpsy</th>\n",
       "      <th>lianjia2</th>\n",
       "      <th>geometry</th>\n",
       "      <th>nearest_community_indices</th>\n",
       "      <th>nearest_community_distances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>链家(潘家园店)</td>\n",
       "      <td>生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所</td>\n",
       "      <td>116.456334</td>\n",
       "      <td>39.875065</td>\n",
       "      <td>链家(潘家园店)</td>\n",
       "      <td>POINT (116.456333588003 39.8750648736051)</td>\n",
       "      <td>[3016, 870]</td>\n",
       "      <td>[0.017490703063189963, 0.05503946806844407]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>链家(团结湖店)</td>\n",
       "      <td>生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所</td>\n",
       "      <td>116.461314</td>\n",
       "      <td>39.927974</td>\n",
       "      <td>链家(团结湖店)</td>\n",
       "      <td>POINT (116.461313947982 39.9279739298663)</td>\n",
       "      <td>[1591, 1578]</td>\n",
       "      <td>[0.19038161099272874, 0.23991664301711862]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>链家(团结湖路)</td>\n",
       "      <td>生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所</td>\n",
       "      <td>116.461018</td>\n",
       "      <td>39.926613</td>\n",
       "      <td>链家(团结湖路)</td>\n",
       "      <td>POINT (116.461018145355 39.9266132293729)</td>\n",
       "      <td>[1591, 1594]</td>\n",
       "      <td>[0.08929879133855328, 0.14133020776835956]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>链家(金星园店)</td>\n",
       "      <td>生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所</td>\n",
       "      <td>116.449045</td>\n",
       "      <td>39.966118</td>\n",
       "      <td>链家(金星园店)</td>\n",
       "      <td>POINT (116.449045305641 39.9661181413891)</td>\n",
       "      <td>[4814, 2183]</td>\n",
       "      <td>[0.023880360645829844, 0.13137326907335756]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>链家(花家地西里店)</td>\n",
       "      <td>生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所</td>\n",
       "      <td>116.451689</td>\n",
       "      <td>39.987442</td>\n",
       "      <td>链家(花家地西里店)</td>\n",
       "      <td>POINT (116.451689294448 39.987442214481696)</td>\n",
       "      <td>[519, 4217]</td>\n",
       "      <td>[0.22282636588868104, 0.26696901573145504]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name                               type        gpsx       gpsy  \\\n",
       "0    链家(潘家园店)  生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所  116.456334  39.875065   \n",
       "1    链家(团结湖店)  生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所  116.461314  39.927974   \n",
       "2    链家(团结湖路)  生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所  116.461018  39.926613   \n",
       "3    链家(金星园店)  生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所  116.449045  39.966118   \n",
       "4  链家(花家地西里店)  生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所  116.451689  39.987442   \n",
       "\n",
       "     lianjia2                                     geometry  \\\n",
       "0    链家(潘家园店)    POINT (116.456333588003 39.8750648736051)   \n",
       "1    链家(团结湖店)    POINT (116.461313947982 39.9279739298663)   \n",
       "2    链家(团结湖路)    POINT (116.461018145355 39.9266132293729)   \n",
       "3    链家(金星园店)    POINT (116.449045305641 39.9661181413891)   \n",
       "4  链家(花家地西里店)  POINT (116.451689294448 39.987442214481696)   \n",
       "\n",
       "  nearest_community_indices                  nearest_community_distances  \n",
       "0               [3016, 870]  [0.017490703063189963, 0.05503946806844407]  \n",
       "1              [1591, 1578]   [0.19038161099272874, 0.23991664301711862]  \n",
       "2              [1591, 1594]   [0.08929879133855328, 0.14133020776835956]  \n",
       "3              [4814, 2183]  [0.023880360645829844, 0.13137326907335756]  \n",
       "4               [519, 4217]   [0.22282636588868104, 0.26696901573145504]  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communities_gdf = dataframes_list[0]\n",
    "df = pd.read_csv('nearest_community/' + communities_gdf['chinese_csv'].unique()[0])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_dataframe = pd.DataFrame()\n",
    "for i in range(0, len(dataframes_list)):\n",
    "    communities_gdf = pd.DataFrame(dataframes_list[i])\n",
    "    \n",
    "    df = pd.read_csv('nearest_community/' + communities_gdf['chinese_csv'].unique()[0])\n",
    "    \n",
    "    communities_gdf['nearest_index_1'] = 0\n",
    "    communities_gdf['nearest_index_2'] = 0\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        nearest_indices_str = row['nearest_community_indices']\n",
    "        nearest_indices = [int(num) for num in re.findall(r'\\d+', nearest_indices_str)]\n",
    "        \n",
    "        # Update communities_gdf for the first nearest community\n",
    "        if len(nearest_indices) > 0 and nearest_indices[0] in communities_gdf.index:\n",
    "            communities_gdf.loc[nearest_indices[0], 'nearest_index_1'] = 1\n",
    "            communities_gdf.loc[nearest_indices[0], 'nearest_index_2'] = 1\n",
    "        \n",
    "        # Update communities_gdf for the second nearest community\n",
    "        if len(nearest_indices) > 1 and nearest_indices[1] in communities_gdf.index:\n",
    "            communities_gdf.loc[nearest_indices[1], 'nearest_index_2'] = 1\n",
    "    \n",
    "    revised_dataframe = pd.concat([communities_gdf, revised_dataframe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_dataframe.to_csv('cleaned_district_Jan_3.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate each community's distance to each store\n",
    "\n",
    "the cpp file calculates each community's smallest distance to the stores\n",
    "\n",
    "```\n",
    "g++ -O3 -shared -std=c++11 -fPIC -I/usr/include/python3.8 -I/home/xuyuan/.local/lib/python3.8/site-packages/pybind11/include -o nearest_stores.so nearest_stores.cpp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_district_Jan_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list = []\n",
    "for i in data['chinese_csv'].unique():\n",
    "    dataframes_list.append(data[data['chinese_csv'] == i])\n",
    "\n",
    "for i in range(len(dataframes_list)):\n",
    "    dataframes_list[i] = gpd.GeoDataFrame(dataframes_list[i], geometry=dataframes_list[i]['geometry'].apply(wkt.loads))\n",
    "    dataframes_list[i] = dataframes_list[i].set_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nearest_stores\n",
    "\n",
    "result_dataframe = pd.DataFrame()\n",
    "\n",
    "for i in range(0, len(dataframes_list)):\n",
    "    communities_gdf = dataframes_list[i]\n",
    "    community_locations_cpp = [list(point.coords[0]) for point in communities_gdf['geometry']]\n",
    "    df = pd.read_csv('lianjia_beke/' + communities_gdf['chinese_csv'].unique()[0])\n",
    "    store_locations_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['gpsx'], df['gpsy']))\n",
    "    store_locations_cpp = [list(point.coords[0]) for point in store_locations_gdf['geometry']]\n",
    "    \n",
    "    num_nearest_communities = 1 # you may change this number whatever you want\n",
    "    nearest_store_indices_tuple = \\\n",
    "        nearest_stores.find_nearest_stores_to_communities(store_locations_cpp, community_locations_cpp, num_nearest_communities)\n",
    "    \n",
    "    nearest_store_indices, nearest_store_distances = nearest_store_indices_tuple\n",
    "\n",
    "    # Assign the list to the 'nearest_community_indices' column\n",
    "    communities_gdf['nearest_store_indices'] = nearest_store_indices\n",
    "    communities_gdf['nearest_store_distances'] = nearest_store_distances\n",
    "    \n",
    "    communities_gdf = pd.DataFrame(communities_gdf)\n",
    "    \n",
    "    result_dataframe = pd.concat([communities_gdf, result_dataframe])\n",
    "    # store_locations_gdf.to_csv('nearest_community/' + communities_gdf['chinese_csv'].unique()[0], index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we are working with nearest distance, we can directly convert it to double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe['nearest_store_distances'] = result_dataframe['nearest_store_distances'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe.to_csv('cleaned_district_Jan_4.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## removing temperary files\n",
    "\n",
    "the following command remove all temperary files generated during our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_district_Jan.csv has been deleted.\n",
      "cleaned_district_Jan_2.csv has been deleted.\n",
      "cleaned_district_Jan_3.csv has been deleted.\n"
     ]
    }
   ],
   "source": [
    "def delete_file(filename):\n",
    "    try:\n",
    "        os.remove(filename)\n",
    "        print(f\"{filename} has been deleted.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File: {filename} not found.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Replace 'my_file.txt' with the name of the file you want to delete\n",
    "delete_file('cleaned_district_Jan.csv')\n",
    "delete_file('cleaned_district_Jan_2.csv')\n",
    "delete_file('cleaned_district_Jan_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_district_Jan_4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now you need to refer to the classifying dataframe\n",
    "\n",
    "after you have done that dataframe, you would obtain a cleaned_district_Jan_5.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_district_Jan_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by = ['id', 'year'], inplace = True)\n",
    "data.reset_index(drop = True, inplace = True)\n",
    "data['lag_lianjia'] = data.groupby('id')['lianjia_420'].shift(1)\n",
    "data['lag_lianjia'] = data['lag_lianjia'].fillna(data['lianjia_420'])\n",
    "data[['id', 'lianjia_420', 'lag_lianjia']]\n",
    "data['potential_entry'] = (data['lianjia_420'] > data['lag_lianjia']).astype(int)\n",
    "\n",
    "# Initialize the 'entry' column with zeros (no entry)\n",
    "data['entry'] = 0\n",
    "\n",
    "# For each group (each 'id'), find the first occurrence where 'potential_entry' is 1 and mark it as the entry\n",
    "for id_val in data['id'].unique():\n",
    "    first_entry_index = data[(data['id'] == id_val) & (data['potential_entry'] == 1)].index.min()\n",
    "    if pd.notna(first_entry_index):\n",
    "        data.at[first_entry_index, 'entry'] = 1\n",
    "data.drop('potential_entry', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "for i in range(1, n + 1):\n",
    "    data[f'post{i}'] = data.groupby('id')['entry'].shift(i).fillna(0)\n",
    "for i in range(1, n + 1):\n",
    "    data[f'pre{i}'] = data.groupby('id')['entry'].shift(-i).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lag_lianjia1'] = data.groupby('id')['lianjia_5'].shift(1)\n",
    "data['lag_lianjia1'] = data['lag_lianjia1'].fillna(data['lianjia_5'])\n",
    "data[['id', 'lianjia_5', 'lag_lianjia']]\n",
    "\n",
    "\n",
    "data['potential_entry'] = (data['lianjia_5'] > data['lag_lianjia1']).astype(int)\n",
    "\n",
    "# Initialize the 'entry' column with zeros (no entry)\n",
    "data['entry1'] = 0\n",
    "\n",
    "# For each group (each 'id'), find the first occurrence where 'potential_entry' is 1 and mark it as the entry\n",
    "for id_val in data['id'].unique():\n",
    "    first_entry_index = data[(data['id'] == id_val) & (data['potential_entry'] == 1)].index.min()\n",
    "    if pd.notna(first_entry_index):\n",
    "        data.at[first_entry_index, 'entry1'] = 1\n",
    "data.drop('potential_entry', axis=1, inplace=True)\n",
    "\n",
    "n = 3\n",
    "for i in range(1, n + 1):\n",
    "    data[f'postr{i}'] = data.groupby('id')['entry1'].shift(i).fillna(0)\n",
    "for i in range(1, n + 1):\n",
    "    data[f'prer{i}'] = data.groupby('id')['entry1'].shift(-i).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the idea should be if the lianjia has been in the market before our study happens, then we should not give it a effect of entry. \n",
    "\n",
    "We just observe the entry effect, and we do not care about the already built in effect.\n",
    "\n",
    "Besides, the data is so confused, and there are multiple periods before and after the entry, how to deal with them. Since this is a fuzzy DID like result, that the reference groups is hard to identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.to_csv('cleaned_district_Jan_6.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
