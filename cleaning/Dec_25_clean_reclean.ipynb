{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuyuan/.local/lib/python3.8/site-packages/geopandas/_compat.py:124: UserWarning: The Shapely GEOS version (3.11.2-CAPI-1.17.2) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_25203/3367804660.py:5: DeprecationWarning: Shapely 2.0 is installed, but because PyGEOS is also installed, GeoPandas still uses PyGEOS by default. However, starting with version 0.14, the default will switch to Shapely. To force to use Shapely 2.0 now, you can either uninstall PyGEOS or set the environment variable USE_PYGEOS=0. You can do this before starting the Python process, or in your code before importing geopandas:\n",
      "\n",
      "import os\n",
      "os.environ['USE_PYGEOS'] = '0'\n",
      "import geopandas\n",
      "\n",
      "In the next release, GeoPandas will switch to using Shapely by default, even if PyGEOS is installed. If you only have PyGEOS installed to get speed-ups, this switch should be smooth. However, if you are using PyGEOS directly (calling PyGEOS functions on geometries from GeoPandas), this will then stop working and you are encouraged to migrate from PyGEOS to Shapely 2.0 (https://shapely.readthedocs.io/en/latest/migration_pygeos.html).\n",
      "  import geopandas as gpd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "import re\n",
    "\n",
    "os.chdir('../') # set working directory to root of repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we extract the number of transactions within the given 1km geometry to map with the community level data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the below codes are for merging the 1km data to the district level data\n",
    "\n",
    "you do not need to execute them because I have already merged it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_1km.csv')\n",
    "df = pd.read_csv('uncleaned_district.csv')\n",
    "data.rename(columns={'prft':'district_income', 'num': 'district_num', 'price': 'district_price'}, inplace=True)\n",
    "data.drop(columns = 'index', inplace=True)\n",
    "\n",
    "# Create GeoDataFrames\n",
    "df_copy = gpd.GeoDataFrame(df.copy(), geometry=df['geometry'].apply(wkt.loads))\n",
    "data_copy = gpd.GeoDataFrame(data.copy(), geometry=data['geometry'].apply(wkt.loads))\n",
    "des = gpd.GeoDataFrame(data.drop_duplicates(subset=['geometry'], keep='first').copy(), \n",
    "                       geometry=data.drop_duplicates(subset=['geometry'], keep='first')['geometry'].apply(wkt.loads))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'year', 'top', 'left', 'id', 'right', 'bottom', 'ngtm',\n",
       "       'living', 'flnum', 'wtpp', 'tihu', 'pripe', 'mall', 'ldtm', 'numbu',\n",
       "       'diff', 'west', 'kind', 'hotel', 'muse', 'super', 'nego', 'jd',\n",
       "       'grrate', 'prim', 'broke', 'dfrat', 'district_income', 'flrate',\n",
       "       'reside', 'district_num', 'wttm', 'toilet', 'ktv', 'bedroo', 'peri',\n",
       "       'park', 'lttm', 'mid', 'district_price', 'kitche', 'old', 'area',\n",
       "       'fllvl', 'age', 'sub', 'lianj', 'light', 'pop', 'pm25', 'geometry',\n",
       "       'city', 'id_unique', 'point_value', 'grppr', 'grpnu', 'grpic'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['building_type', 'village', 'district', 'floor_level', 'new_lng',\n",
       "       'new_lat', 'year', 'floor_ratio', 'green_ratio', 'nego_times',\n",
       "       'lead_times', 'total_building', 'total_resident', 'watching_people',\n",
       "       'watched_times', 'striker_price', 'striker_price_pers', 'end_price',\n",
       "       'end_price_pers', 'area', 'nego_period', 'bedroom', 'living_room',\n",
       "       'kitchen', 'toilet', 'total_floor_number', 'elevator_ratio',\n",
       "       'house_age', 'income', 'number', 'super', 'sub', 'hotel', 'kind',\n",
       "       'prim', 'mid', 'shop_mall', 'west_food', 'park', 'museum', 'ktv',\n",
       "       'jiadian', 'old', 'other', 'other_5', 'lianjia', 'lianjia_5', 'beke',\n",
       "       'beke_5', 'geometry', 'light', 'pop', 'pm25', 'region', 'id',\n",
       "       'business_area'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df_copy.set_crs('epsg:4326')\n",
    "data_copy = data_copy.set_crs('epsg:4326')\n",
    "des = des.set_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_copy['district_price'] = data_copy['district_price'] / data_copy['area']\n",
    "# this would guarantee that the result is the average price per square meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuyuan/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    }
   ],
   "source": [
    "if 'index_left' in df_copy.columns:\n",
    "    df_copy.drop(columns=['index_left'], inplace=True)\n",
    "if 'index_right' in df_copy.columns:\n",
    "    df_copy.drop(columns=['index_right'], inplace=True)\n",
    "\n",
    "# Check and rename/drop 'index_left' and 'index_right' in des\n",
    "if 'index_left' in des.columns:\n",
    "    des.drop(columns=['index_left'], inplace=True)\n",
    "if 'index_right' in des.columns:\n",
    "    des.drop(columns=['index_right'], inplace=True)\n",
    "    \n",
    "joined_gdf = gpd.sjoin(df_copy, des, how=\"left\", op='within')\n",
    "joined_gdf.drop(columns = ['district_income', 'district_num', 'district_price'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined_gdf['id_unique', 'year'] # id_unique is the unique identification of the data object\n",
    "# and now we shall join the data to the district data\n",
    "\n",
    "data_copy['unique_key'] = data_copy['id_unique'].astype(str) + '_' + data_copy['year'].astype(str)\n",
    "joined_gdf['unique_key'] = joined_gdf['id_unique'].astype(str) + '_' + joined_gdf['year_left'].astype(str)\n",
    "data_relevant = data_copy[['unique_key', 'district_income', 'district_num', 'district_price']]\n",
    "\n",
    "joined_gdf = joined_gdf.merge(data_relevant, on='unique_key', how='left')\n",
    "joined_gdf.drop(columns=['unique_key'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['unique_key'] = df_copy['id'].astype(str) + '_' + df_copy['year'].astype(str)\n",
    "joined_gdf['unique_key'] = joined_gdf['id_left'].astype(str) + '_' + joined_gdf['year_left'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf.rename(columns = {'district_income': 'region_income', 'district_num': 'region_num', 'district_price': 'region_price'}, inplace=True)\n",
    "data_relevant = joined_gdf[['unique_key', 'region_income', 'region_num', 'region_price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df_copy.merge(data_relevant, on='unique_key', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.to_csv('cleaned_district_Jan.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_district_Jan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by = ['id', 'year'], inplace = True)\n",
    "data['lag_lianjia'] = data.groupby('id')['lianjia_5'].shift(1)\n",
    "data['lag_lianjia'] = data['lag_lianjia'].fillna(data['lianjia_5'])\n",
    "data[['id', 'lianjia_5', 'lag_lianjia']]\n",
    "data['entry'] = (data['lianjia_5'] > data['lag_lianjia']).astype(int)\n",
    "\n",
    "### note that this function is wrong!!!!!!!!\n",
    "# and we should just drop it right here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "for i in range(1, n + 1):\n",
    "    data[f'post{i}'] = data.groupby('id')['entry'].shift(i).fillna(0)\n",
    "for i in range(1, n + 1):\n",
    "    data[f'pre{i}'] = data.groupby('id')['entry'].shift(-i).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('cleaned_district_Jan_2.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now we construct the RD design dataset\n",
    "\n",
    "the RD design dataset contains the following properties:\n",
    "\n",
    "First, we extract a list of csv files located in the lianjia_beke directory within the given working path. Then we map these files to the design of communities and extract each lianjia store with its nearest community or nearest two communities respectively. Then we conduct the RD analysis in the stata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_district_Jan_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the directory path\n",
    "directory_path = \"lianjia_beke\"\n",
    "filenames = []\n",
    "# List all files and directories in the given path\n",
    "for filename in os.listdir(directory_path):\n",
    "    filenames.append(filename)\n",
    "# data['region'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_name_mapping = {\n",
    "    'beijing': '北京市',\n",
    "    'chengdu': '成都市',\n",
    "    'chongqing': '重庆市',\n",
    "    'guangzhou': '广州市',\n",
    "    'hangzhou': '杭州市',\n",
    "    'nanjing': '南京市',\n",
    "    'shanghai': '上海市',\n",
    "    'shenzhen': '深圳市',\n",
    "    'tianjin': '天津市',\n",
    "    'wuhan': '武汉市',\n",
    "    'xian': '西安市'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_chinese_csv(row):\n",
    "    year_suffix = str(row['year'])[-2:]  # Extract the last two digits of the year\n",
    "    chinese_city = city_name_mapping[row['region']]  # Map to Chinese city name\n",
    "    return f\"{chinese_city}{year_suffix}.csv\"  # Combine to form the Chinese CSV file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['chinese_csv'] = data.apply(map_to_chinese_csv, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list = []\n",
    "for i in data['chinese_csv'].unique():\n",
    "    dataframes_list.append(data[data['chinese_csv'] == i])\n",
    "\n",
    "for i in range(len(dataframes_list)):\n",
    "    dataframes_list[i] = gpd.GeoDataFrame(dataframes_list[i], geometry=dataframes_list[i]['geometry'].apply(wkt.loads))\n",
    "    dataframes_list[i] = dataframes_list[i].set_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE\n",
    "\n",
    "the code below extract the number of nearest community for each lianjia's store. It used nested for loop to achieve this and this code runs pretty long time, we can optimize it using Cpp.\n",
    "\n",
    "For computing purpose, I suppose to use the Cpp codes in the second block\n",
    "\n",
    "procedure to build it (you should revise the location in your computer):\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "pip install pybind11\n",
    "\n",
    "python3-config --cflags\n",
    "\n",
    "pybind11-config --includes\n",
    "\n",
    "cd the/file/path/RealEstateBrokerage/\n",
    "\n",
    "g++ -O3 -shared -std=c++11 -fPIC -I/usr/include/python3.8 -I/home/xuyuan/.local/lib/python3.8/site-packages/pybind11/include -o nearest_community_cpp.so nearest_community_cpp.cpp\n",
    "```\n",
    "\n",
    "the running time is less than 25 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for i in range(0, len(dataframes_list)):\n",
    "    communities_gdf = dataframes_list[i]\n",
    "    df = pd.read_csv('lianjia_beke/' + communities_gdf['chinese_csv'].unique()[0])\n",
    "    store_locations_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['gpsx'], df['gpsy']))\n",
    "    # store_locations_gdf['nearest_community_index'] = -1\n",
    "    \n",
    "    num_nearest_communities = 2\n",
    "    \n",
    "    nearest_community_indices_list = []\n",
    "    \n",
    "    for store_index, store_row in store_locations_gdf.iterrows():\n",
    "        store_location = store_row['geometry']\n",
    "        \n",
    "        # Calculate distances to all communities and store them in a Series\n",
    "        distances = communities_gdf.geometry.apply(lambda x: store_location.distance(x))\n",
    "        \n",
    "        # Sort the distances and select the indices of the nearest communities\n",
    "        nearest_community_indices = distances.argsort()[:num_nearest_communities].tolist()\n",
    "        \n",
    "        # Append the list of nearest community indices to the list\n",
    "        nearest_community_indices_list.append(nearest_community_indices)\n",
    "        \n",
    "    # Assign the list to the 'nearest_community_indices' column\n",
    "    store_locations_gdf['nearest_community_indices'] = nearest_community_indices_list\n",
    "    \n",
    "    store_locations_gdf.to_csv('nearest_community/' + communities_gdf['chinese_csv'].unique()[0], index = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nearest_community_cpp\n",
    "\n",
    "directory_path = \"nearest_community\"\n",
    "if not os.path.exists(directory_path):\n",
    "    # If it doesn't exist, create the directory\n",
    "    os.makedirs(directory_path)\n",
    "\n",
    "for i in range(0, len(dataframes_list)):\n",
    "    communities_gdf = dataframes_list[i]\n",
    "    community_locations_cpp = [list(point.coords[0]) for point in communities_gdf['geometry']]\n",
    "    df = pd.read_csv('lianjia_beke/' + communities_gdf['chinese_csv'].unique()[0])\n",
    "    store_locations_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['gpsx'], df['gpsy']))\n",
    "    store_locations_cpp = [list(point.coords[0]) for point in store_locations_gdf['geometry']]\n",
    "    \n",
    "    num_nearest_communities = 2 # you may change this number whatever you want\n",
    "    nearest_community_indices_tuple = \\\n",
    "        nearest_community_cpp.find_nearest_communities(store_locations_cpp, community_locations_cpp, num_nearest_communities)\n",
    "    \n",
    "    nearest_community_indices_list, nearest_community_distances_list = nearest_community_indices_tuple\n",
    "\n",
    "    # Assign the list to the 'nearest_community_indices' column\n",
    "    store_locations_gdf['nearest_community_indices'] = nearest_community_indices_list\n",
    "    store_locations_gdf['nearest_community_distances'] = nearest_community_distances_list\n",
    "    \n",
    "    store_locations_gdf.to_csv('nearest_community/' + communities_gdf['chinese_csv'].unique()[0], index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we merge it back to our dataframe to get the original dataframe. Note that the order gonna be:\n",
    "\n",
    "1. The index of the nearest community to the store location.\n",
    "2. The index of the second nearest community to the store location.\n",
    "3. If there are more communities to consider (i.e., num_nearest_communities is greater than 2), the indices of the subsequent nearest communities will follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>gpsx</th>\n",
       "      <th>gpsy</th>\n",
       "      <th>lianjia2</th>\n",
       "      <th>geometry</th>\n",
       "      <th>nearest_community_indices</th>\n",
       "      <th>nearest_community_distances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>链家(潘家园店)</td>\n",
       "      <td>生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所</td>\n",
       "      <td>116.456334</td>\n",
       "      <td>39.875065</td>\n",
       "      <td>链家(潘家园店)</td>\n",
       "      <td>POINT (116.456333588003 39.8750648736051)</td>\n",
       "      <td>[3016, 870]</td>\n",
       "      <td>[0.017490703063189963, 0.05503946806844407]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>链家(团结湖店)</td>\n",
       "      <td>生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所</td>\n",
       "      <td>116.461314</td>\n",
       "      <td>39.927974</td>\n",
       "      <td>链家(团结湖店)</td>\n",
       "      <td>POINT (116.461313947982 39.9279739298663)</td>\n",
       "      <td>[1591, 1578]</td>\n",
       "      <td>[0.19038161099272874, 0.23991664301711862]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>链家(团结湖路)</td>\n",
       "      <td>生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所</td>\n",
       "      <td>116.461018</td>\n",
       "      <td>39.926613</td>\n",
       "      <td>链家(团结湖路)</td>\n",
       "      <td>POINT (116.461018145355 39.9266132293729)</td>\n",
       "      <td>[1591, 1594]</td>\n",
       "      <td>[0.08929879133855328, 0.14133020776835956]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>链家(金星园店)</td>\n",
       "      <td>生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所</td>\n",
       "      <td>116.449045</td>\n",
       "      <td>39.966118</td>\n",
       "      <td>链家(金星园店)</td>\n",
       "      <td>POINT (116.449045305641 39.9661181413891)</td>\n",
       "      <td>[4814, 2183]</td>\n",
       "      <td>[0.023880360645829844, 0.13137326907335756]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>链家(花家地西里店)</td>\n",
       "      <td>生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所</td>\n",
       "      <td>116.451689</td>\n",
       "      <td>39.987442</td>\n",
       "      <td>链家(花家地西里店)</td>\n",
       "      <td>POINT (116.451689294448 39.987442214481696)</td>\n",
       "      <td>[519, 4217]</td>\n",
       "      <td>[0.22282636588868104, 0.26696901573145504]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name                               type        gpsx       gpsy  \\\n",
       "0    链家(潘家园店)  生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所  116.456334  39.875065   \n",
       "1    链家(团结湖店)  生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所  116.461314  39.927974   \n",
       "2    链家(团结湖路)  生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所  116.461018  39.926613   \n",
       "3    链家(金星园店)  生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所  116.449045  39.966118   \n",
       "4  链家(花家地西里店)  生活服务;中介机构;中介机构|购物服务;购物相关场所;购物相关场所  116.451689  39.987442   \n",
       "\n",
       "     lianjia2                                     geometry  \\\n",
       "0    链家(潘家园店)    POINT (116.456333588003 39.8750648736051)   \n",
       "1    链家(团结湖店)    POINT (116.461313947982 39.9279739298663)   \n",
       "2    链家(团结湖路)    POINT (116.461018145355 39.9266132293729)   \n",
       "3    链家(金星园店)    POINT (116.449045305641 39.9661181413891)   \n",
       "4  链家(花家地西里店)  POINT (116.451689294448 39.987442214481696)   \n",
       "\n",
       "  nearest_community_indices                  nearest_community_distances  \n",
       "0               [3016, 870]  [0.017490703063189963, 0.05503946806844407]  \n",
       "1              [1591, 1578]   [0.19038161099272874, 0.23991664301711862]  \n",
       "2              [1591, 1594]   [0.08929879133855328, 0.14133020776835956]  \n",
       "3              [4814, 2183]  [0.023880360645829844, 0.13137326907335756]  \n",
       "4               [519, 4217]   [0.22282636588868104, 0.26696901573145504]  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "communities_gdf = dataframes_list[0]\n",
    "df = pd.read_csv('nearest_community/' + communities_gdf['chinese_csv'].unique()[0])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_dataframe = pd.DataFrame()\n",
    "for i in range(0, len(dataframes_list)):\n",
    "    communities_gdf = pd.DataFrame(dataframes_list[i])\n",
    "    \n",
    "    df = pd.read_csv('nearest_community/' + communities_gdf['chinese_csv'].unique()[0])\n",
    "    \n",
    "    communities_gdf['nearest_index_1'] = 0\n",
    "    communities_gdf['nearest_index_2'] = 0\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        nearest_indices_str = row['nearest_community_indices']\n",
    "        nearest_indices = [int(num) for num in re.findall(r'\\d+', nearest_indices_str)]\n",
    "        \n",
    "        # Update communities_gdf for the first nearest community\n",
    "        if len(nearest_indices) > 0 and nearest_indices[0] in communities_gdf.index:\n",
    "            communities_gdf.loc[nearest_indices[0], 'nearest_index_1'] = 1\n",
    "            communities_gdf.loc[nearest_indices[0], 'nearest_index_2'] = 1\n",
    "        \n",
    "        # Update communities_gdf for the second nearest community\n",
    "        if len(nearest_indices) > 1 and nearest_indices[1] in communities_gdf.index:\n",
    "            communities_gdf.loc[nearest_indices[1], 'nearest_index_2'] = 1\n",
    "    \n",
    "    revised_dataframe = pd.concat([communities_gdf, revised_dataframe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_dataframe.to_csv('cleaned_district_Jan_3.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate each community's distance to each store\n",
    "\n",
    "the cpp file calculates each community's smallest distance to the stores\n",
    "\n",
    "```\n",
    "g++ -O3 -shared -std=c++11 -fPIC -I/usr/include/python3.8 -I/home/xuyuan/.local/lib/python3.8/site-packages/pybind11/include -o nearest_stores.so nearest_stores.cpp\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_district_Jan_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list = []\n",
    "for i in data['chinese_csv'].unique():\n",
    "    dataframes_list.append(data[data['chinese_csv'] == i])\n",
    "\n",
    "for i in range(len(dataframes_list)):\n",
    "    dataframes_list[i] = gpd.GeoDataFrame(dataframes_list[i], geometry=dataframes_list[i]['geometry'].apply(wkt.loads))\n",
    "    dataframes_list[i] = dataframes_list[i].set_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nearest_stores\n",
    "\n",
    "result_dataframe = pd.DataFrame()\n",
    "\n",
    "for i in range(0, len(dataframes_list)):\n",
    "    communities_gdf = dataframes_list[i]\n",
    "    community_locations_cpp = [list(point.coords[0]) for point in communities_gdf['geometry']]\n",
    "    df = pd.read_csv('lianjia_beke/' + communities_gdf['chinese_csv'].unique()[0])\n",
    "    store_locations_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['gpsx'], df['gpsy']))\n",
    "    store_locations_cpp = [list(point.coords[0]) for point in store_locations_gdf['geometry']]\n",
    "    \n",
    "    num_nearest_communities = 1 # you may change this number whatever you want\n",
    "    nearest_store_indices_tuple = \\\n",
    "        nearest_stores.find_nearest_stores_to_communities(store_locations_cpp, community_locations_cpp, num_nearest_communities)\n",
    "    \n",
    "    nearest_store_indices, nearest_store_distances = nearest_store_indices_tuple\n",
    "\n",
    "    # Assign the list to the 'nearest_community_indices' column\n",
    "    communities_gdf['nearest_store_indices'] = nearest_store_indices\n",
    "    communities_gdf['nearest_store_distances'] = nearest_store_distances\n",
    "    \n",
    "    communities_gdf = pd.DataFrame(communities_gdf)\n",
    "    \n",
    "    result_dataframe = pd.concat([communities_gdf, result_dataframe])\n",
    "    # store_locations_gdf.to_csv('nearest_community/' + communities_gdf['chinese_csv'].unique()[0], index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we are working with nearest distance, we can directly convert it to double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe['nearest_store_distances'] = result_dataframe['nearest_store_distances'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe.to_csv('cleaned_district_Jan_4.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## removing temperary files\n",
    "\n",
    "the following command remove all temperary files generated during our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_district_Jan.csv has been deleted.\n",
      "cleaned_district_Jan_2.csv has been deleted.\n",
      "cleaned_district_Jan_3.csv has been deleted.\n"
     ]
    }
   ],
   "source": [
    "def delete_file(filename):\n",
    "    try:\n",
    "        os.remove(filename)\n",
    "        print(f\"{filename} has been deleted.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File: {filename} not found.\")\n",
    "    except PermissionError:\n",
    "        print(f\"Permission denied.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Replace 'my_file.txt' with the name of the file you want to delete\n",
    "delete_file('cleaned_district_Jan.csv')\n",
    "delete_file('cleaned_district_Jan_2.csv')\n",
    "delete_file('cleaned_district_Jan_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_district_Jan_4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now you need to refer to the classifying dataframe\n",
    "\n",
    "after you have done that dataframe, you would obtain a cleaned_district_Jan_5.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_district_Jan_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by = ['id', 'year'], inplace = True)\n",
    "data.reset_index(drop = True, inplace = True)\n",
    "data['lag_lianjia'] = data.groupby('id')['lianjia_410'].shift(1)\n",
    "data['lag_lianjia'] = data['lag_lianjia'].fillna(data['lianjia_410'])\n",
    "data[['id', 'lianjia_410', 'lag_lianjia']]\n",
    "data['potential_entry'] = (data['lianjia_410'] > data['lag_lianjia']).astype(int)\n",
    "\n",
    "# Initialize the 'entry' column with zeros (no entry)\n",
    "data['entry'] = 0\n",
    "\n",
    "# For each group (each 'id'), find the first occurrence where 'potential_entry' is 1 and mark it as the entry\n",
    "for id_val in data['id'].unique():\n",
    "    first_entry_index = data[(data['id'] == id_val) & (data['potential_entry'] == 1)].index.min()\n",
    "    if pd.notna(first_entry_index):\n",
    "        data.at[first_entry_index, 'entry'] = 1\n",
    "data.drop('potential_entry', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "for i in range(1, n + 1):\n",
    "    data[f'post{i}'] = data.groupby('id')['entry'].shift(i).fillna(0)\n",
    "for i in range(1, n + 1):\n",
    "    data[f'pre{i}'] = data.groupby('id')['entry'].shift(-i).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lag_lianjia1'] = data.groupby('id')['lianjia_5'].shift(1)\n",
    "data['lag_lianjia1'] = data['lag_lianjia1'].fillna(data['lianjia_5'])\n",
    "data[['id', 'lianjia_5', 'lag_lianjia']]\n",
    "\n",
    "\n",
    "data['potential_entry'] = (data['lianjia_5'] > data['lag_lianjia1']).astype(int)\n",
    "\n",
    "# Initialize the 'entry' column with zeros (no entry)\n",
    "data['entry1'] = 0\n",
    "\n",
    "# For each group (each 'id'), find the first occurrence where 'potential_entry' is 1 and mark it as the entry\n",
    "for id_val in data['id'].unique():\n",
    "    first_entry_index = data[(data['id'] == id_val) & (data['potential_entry'] == 1)].index.min()\n",
    "    if pd.notna(first_entry_index):\n",
    "        data.at[first_entry_index, 'entry1'] = 1\n",
    "data.drop('potential_entry', axis=1, inplace=True)\n",
    "\n",
    "n = 3\n",
    "for i in range(1, n + 1):\n",
    "    data[f'postr{i}'] = data.groupby('id')['entry1'].shift(i).fillna(0)\n",
    "for i in range(1, n + 1):\n",
    "    data[f'prer{i}'] = data.groupby('id')['entry1'].shift(-i).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the idea should be if the lianjia has been in the market before our study happens, then we should not give it a effect of entry. \n",
    "\n",
    "We just observe the entry effect, and we do not care about the already built in effect.\n",
    "\n",
    "Besides, the data is so confused, and there are multiple periods before and after the entry, how to deal with them. Since this is a fuzzy DID like result, that the reference groups is hard to identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('cleaned_district_Jan_6.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_district_Jan_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['price_concession'] = (data['end_price_pers'] - data['striker_price_pers']).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Descriptive Statistics:\n",
      "count    2.228370e+05\n",
      "mean     7.469944e+03\n",
      "std      1.894920e+06\n",
      "min      0.000000e+00\n",
      "25%      3.355000e+02\n",
      "50%      7.750000e+02\n",
      "75%      1.670000e+03\n",
      "max      7.715398e+08\n",
      "Name: price_concession, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic Descriptive Statistics:\")\n",
    "print(data['price_concession'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['price_concession'] = data['price_concession'] / data['striker_price_pers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Descriptive Statistics:\n",
      "count    222837.000000\n",
      "mean          0.163312\n",
      "std          26.200336\n",
      "min           0.000000\n",
      "25%           0.016469\n",
      "50%           0.027910\n",
      "75%           0.042468\n",
      "max        9998.961121\n",
      "Name: price_concession, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic Descriptive Statistics:\")\n",
    "print(data['price_concession'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('cleaned_district_Jan_7.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new cleaning\n",
    "\n",
    "1. we first need to extract the given brokerages from the cleaned and then we need to count the all brokerages in each 410 meters of the districts\n",
    "2. then we construct the dummy variable if it contains the multiple brands in each region\n",
    "3. and then perform DID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_plate = ['链家|德佑|伊诚', '链家|德佑|伊诚', '链家|德佑|伊诚|富房|远投|置家|优铭家|链胜|欢家居|邦房|稻家|福居客|居安易|欢家居|爱享家|壳点|仁道|朝晖|育贤鼎|晟祺|珺利达', '链家|德佑|伊诚|富房|远投|21世纪|中环|置家|优铭家|大屋|优屋|玖卓|链胜|品信|欢家居|邦房|稻家|福居客|居安易|爱享家|壳点', '链家|德佑|伊诚|富房|远投|21世纪|中环|置家|凯城|住天下|珍房源|添房置业|览众|糯家|脉房|68置业|微地产|优铭家|万福|华仁|大屋', '链家|德佑|伊诚|富房|远投|21世纪|中环|宏盛发|住商|置家|众誉|凯城|住天下|珍房源|添房|览众|糯家|脉房|68置业|微地产|优铭家', '链家|德佑|伊诚|富房|远投|21世纪|中环|宏盛发|住商|置家|众誉|凯城|住天下|珍房源|添房|览众|糯家|脉房|68置业|微地产|优铭家']\n",
    "\n",
    "unique_values = set('|'.join(name_plate).split('|'))\n",
    "unique_values_list = list(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '|'.join(unique_values_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21_上海市.csv 1613\n",
      "17_西安市.csv 74\n",
      "18_重庆市.csv 307\n",
      "22_广州市.csv 1763\n",
      "19_深圳市.csv 473\n",
      "16_西安市.csv 61\n",
      "16_成都市.csv 811\n",
      "20_重庆市.csv 886\n",
      "22_南京市.csv 1331\n",
      "18_上海市.csv 1890\n",
      "17_广州市.csv 362\n",
      "16_杭州市.csv 207\n",
      "22_重庆市.csv 1492\n",
      "18_杭州市.csv 391\n",
      "17_杭州市.csv 361\n",
      "20_南京市.csv 1035\n",
      "18_成都市.csv 1171\n",
      "17_北京市.csv 1408\n",
      "19_杭州市.csv 503\n",
      "19_天津市.csv 620\n",
      "22_成都市.csv 2506\n",
      "17_上海市.csv 1890\n",
      "16_广州市.csv 138\n",
      "20_武汉市.csv 1601\n",
      "20_深圳市.csv 1153\n",
      "18_西安市.csv 126\n",
      "16_天津市.csv 314\n",
      "16_南京市.csv 337\n",
      "22_上海市.csv 1660\n",
      "16_北京市.csv 1079\n",
      "18_南京市.csv 701\n",
      "21_杭州市.csv 1189\n",
      "18_广州市.csv 383\n",
      "22_武汉市.csv 2143\n",
      "20_杭州市.csv 1014\n",
      "22_天津市.csv 1710\n",
      "21_南京市.csv 1293\n",
      "19_上海市.csv 1838\n",
      "18_天津市.csv 440\n",
      "21_天津市.csv 1643\n",
      "16_深圳市.csv 333\n",
      "17_天津市.csv 429\n",
      "21_广州市.csv 1648\n",
      "19_北京市.csv 1529\n",
      "22_深圳市.csv 1447\n",
      "20_天津市.csv 1327\n",
      "16_重庆市.csv 167\n",
      "19_武汉市.csv 595\n",
      "19_成都市.csv 1645\n",
      "17_重庆市.csv 230\n",
      "19_西安市.csv 323\n",
      "21_深圳市.csv 1383\n",
      "17_南京市.csv 638\n",
      "21_武汉市.csv 2050\n",
      "17_成都市.csv 1046\n",
      "21_成都市.csv 2146\n",
      "19_南京市.csv 806\n",
      "20_成都市.csv 2270\n",
      "20_西安市.csv 985\n",
      "21_重庆市.csv 1405\n",
      "21_西安市.csv 1377\n",
      "16_上海市.csv 1581\n",
      "18_武汉市.csv 441\n",
      "20_广州市.csv 960\n",
      "17_武汉市.csv 343\n",
      "22_杭州市.csv 1160\n",
      "16_武汉市.csv 159\n",
      "18_北京市.csv 1544\n",
      "22_北京市.csv 1863\n",
      "21_北京市.csv 1800\n",
      "22_西安市.csv 1426\n",
      "17_深圳市.csv 569\n",
      "19_重庆市.csv 440\n",
      "20_北京市.csv 1791\n",
      "18_深圳市.csv 579\n",
      "19_广州市.csv 273\n",
      "20_上海市.csv 1489\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'classifying brokerages/cleaned'\n",
    "output_folder_path = 'classifying brokerages/processed'\n",
    "\n",
    "if not os.path.exists(output_folder_path):\n",
    "    os.makedirs(output_folder_path)\n",
    "\n",
    "csv_files = glob.glob(f'{folder_path}/*.csv')\n",
    "for file_path in csv_files:\n",
    "    data = pd.read_csv(file_path)\n",
    "     \n",
    "    mask = data['name'].str.contains(pattern, na=False)\n",
    "    filtered_data = data[mask]\n",
    "    print(os.path.basename(file_path), filtered_data.shape[0])\n",
    "    output_file_path = os.path.join(output_folder_path, os.path.basename(file_path))\n",
    "    filtered_data.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g++ -O3 -shared -std=c++11 -fPIC -I/usr/include/python3.8 -I/home/xuyuan/.local/lib/python3.8/site-packages/pybind11/include -o calculate_nearby_points.so calculate_nearby_points.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = pd.read_csv('cleaned_district_Jan_7.csv')\n",
    "\n",
    "def convert_filename_format(original_filename):\n",
    "    # Split the original filename by '市' and '.csv'\n",
    "    parts = original_filename.split('市')\n",
    "    city = parts[0]  # City name is the part before '市'\n",
    "    year_and_extension = parts[1]  # Year and '.csv' is the part after '市'\n",
    "    \n",
    "    # Extract the year (last two digits) and the extension\n",
    "    year = year_and_extension[:2]\n",
    "    extension = year_and_extension[2:]\n",
    "    \n",
    "    # Combine to form the new filename in 'year_city.csv' format\n",
    "    new_filename = f\"{year}_{city}市{extension}\"\n",
    "    return new_filename\n",
    "\n",
    "original_data['year_city_format'] = [convert_filename_format(filename) for filename in original_data['chinese_csv'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list = []\n",
    "for i in original_data['year_city_format'].unique():\n",
    "    dataframes_list.append(original_data[original_data['year_city_format'] == i])\n",
    "\n",
    "for i in range(len(dataframes_list)):\n",
    "    dataframes_list[i] = gpd.GeoDataFrame(dataframes_list[i], geometry=dataframes_list[i]['geometry'].apply(wkt.loads))\n",
    "    dataframes_list[i] = dataframes_list[i].set_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calculate_nearby_points\n",
    "\n",
    "revised_dataframe = pd.DataFrame()\n",
    "for i in range(0, len(dataframes_list)):\n",
    "    communities_gdf = dataframes_list[i]\n",
    "    df = pd.read_csv('classifying brokerages/processed/' + communities_gdf['year_city_format'].unique()[0])\n",
    "    \n",
    "    store_locations_gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df['gpsx'], df['gpsy']))\n",
    "    \n",
    "    community_locations_cpp = [list(point.coords[0]) for point in communities_gdf['geometry']]\n",
    "    store_locations_cpp = [list(point.coords[0]) for point in store_locations_gdf['geometry']]\n",
    "    \n",
    "    result = calculate_nearby_points.calculate_counts(community_locations_cpp, store_locations_cpp, 410.0)\n",
    "    communities_gdf['all_beke_410'] = result\n",
    "    \n",
    "    revised_dataframe = pd.concat([communities_gdf, revised_dataframe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222837\n",
      "222837\n"
     ]
    }
   ],
   "source": [
    "print(len(revised_dataframe))\n",
    "print(len(original_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_dataframe.sort_values(by = ['id', 'year'], inplace = True)\n",
    "revised_dataframe.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_dataframe.to_csv('cleaned_district_Jan_8.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
