{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pybind11\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import geopandas as gpd\n",
    "import shapely \n",
    "import os\n",
    "import sys\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "import random\n",
    "from geopy.distance import geodesic\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/xuyuan/Desktop/2024 summer/real estate paper/writing/RealEstateBrokerage/network_estimation')\n",
    "import network_formulation\n",
    "os.chdir('/home/xuyuan/Desktop/2024 summer/real estate paper/oritignal cleaning/RealEstateBrokerage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217200\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_stata('template.dta')\n",
    "codebook = {\n",
    "    1: '北京市',\n",
    "    2: '成都市',\n",
    "    3: '重庆市',\n",
    "    4: '广州市',\n",
    "    5: '杭州市',\n",
    "    6: '南京市',\n",
    "    7: '上海市',\n",
    "    8: '深圳市',\n",
    "    9: '天津市',\n",
    "    10: '武汉市'\n",
    "}\n",
    "\n",
    "data['city_id'] = data['city_id'].map(codebook)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_influential_effects(df_network, df_edges):\n",
    "    # Create a graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes for stores and communities\n",
    "    stores = df_network['store_id'].unique()\n",
    "    communities = df_network['community_id'].unique()\n",
    "    for store in stores:\n",
    "        G.add_node(f'Store {store}', type='store')\n",
    "    for community in communities:\n",
    "        G.add_node(f'Community {community}', type='community')\n",
    "\n",
    "    # Add edges based on df_network\n",
    "    for _, row in df_network.iterrows():\n",
    "        store_id = f'Store {row[\"store_id\"]}'\n",
    "        community_id = f'Community {row[\"community_id\"]}'\n",
    "        effect = row['effect']\n",
    "        G.add_edge(store_id, community_id, weight=effect)\n",
    "        # print(f\"Edge added: {store_id} <-> {community_id}, Weight: {effect}\")\n",
    "\n",
    "    # Add edges based on df_edges\n",
    "    for _, row in df_edges.iterrows():\n",
    "        store_id_1 = f'Store {row[\"store_id_1\"]}'\n",
    "        store_id_2 = f'Store {row[\"store_id_2\"]}'\n",
    "        effect = row['effect']\n",
    "        G.add_edge(store_id_1, store_id_2, weight=effect)\n",
    "        # print(f\"Edge added: {store_id_1} <-> {store_id_2}, Weight: {effect}\")\n",
    "\n",
    "    # Initialize community influence\n",
    "    community_influence = {f'Community {i}.0': 0 for i in communities}\n",
    "    # BFS to propagate influence\n",
    "    # Function to propagate influence from a community through the network of stores\n",
    "    def propagate_influence(community):\n",
    "        # print(community)\n",
    "        queue = []\n",
    "        visited = set()\n",
    "\n",
    "        # Initialize the queue with stores directly connected to the community\n",
    "        for store_id in G.neighbors(community):\n",
    "            if store_id.startswith('Store'):\n",
    "                initial_effect = G[community][store_id]['weight']\n",
    "                queue.append((store_id, initial_effect)) # (store_id, cumulative_effect)\n",
    "                visited.add(store_id)\n",
    "                # print(f\"Initial: Community {community} -> Store {store_id}, Effect: {initial_effect}\")\n",
    "\n",
    "\n",
    "        # Perform BFS to propagate the influence\n",
    "        while queue:\n",
    "            current_store, current_effect = queue.pop(0)\n",
    "            for neighbor in G.neighbors(current_store):\n",
    "                if neighbor not in visited:\n",
    "                    if neighbor.startswith('Store'):\n",
    "                        # Calculate the propagated effect\n",
    "                        edge_weight = G[current_store][neighbor]['weight']\n",
    "                        new_effect = current_effect * edge_weight\n",
    "                        queue.append((neighbor, new_effect))\n",
    "                        visited.add(neighbor)\n",
    "                        # print(f\"Propagate: Store {current_store} -> Store {neighbor}, Effect: {new_effect}\")\n",
    "            # Update the influence for the initial community\n",
    "            community_influence[community] += current_effect\n",
    "            # print(f\"Accumulate: Community {community}, Current Store {current_store}, Effect: {current_effect}\")\n",
    "\n",
    "    # Calculate the influence for each community\n",
    "    for community in community_influence.keys():\n",
    "        propagate_influence(community)\n",
    "\n",
    "    return community_influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7627/397515282.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_result = pd.concat([combined_result, merged_data], ignore_index=True, verify_integrity=True)\n",
      "/tmp/ipykernel_7627/397515282.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_result = pd.concat([combined_result, merged_data], ignore_index=True, verify_integrity=True)\n",
      "/tmp/ipykernel_7627/397515282.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_result = pd.concat([combined_result, merged_data], ignore_index=True, verify_integrity=True)\n",
      "/tmp/ipykernel_7627/397515282.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_result = pd.concat([combined_result, merged_data], ignore_index=True, verify_integrity=True)\n",
      "/tmp/ipykernel_7627/397515282.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_result = pd.concat([combined_result, merged_data], ignore_index=True, verify_integrity=True)\n",
      "/tmp/ipykernel_7627/397515282.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_result = pd.concat([combined_result, merged_data], ignore_index=True, verify_integrity=True)\n",
      "/tmp/ipykernel_7627/397515282.py:45: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_result = pd.concat([combined_result, merged_data], ignore_index=True, verify_integrity=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(0, len(data['city_id'].unique())):\n",
    "    for j in range(0, len(data['year'].unique())):\n",
    "        map_data = data[(data['city_id'] == data['city_id'].unique()[i]) & (data['year'] == data['year'].unique()[j])]\n",
    "        map_data = gpd.GeoDataFrame(map_data, geometry = map_data.geometry.apply(wkt.loads))\n",
    "        df = pd.read_csv(\"classifying brokerages/processed/{}_{}.csv\".format(data['year'].unique()[j] - 2000, data['city_id'].unique()[i]))\n",
    "        map_data['longitude'] = map_data['geometry'].apply(lambda point: point.x)\n",
    "        map_data['latitude'] = map_data['geometry'].apply(lambda point: point.y)\n",
    "        \n",
    "        \n",
    "        effects = map_data['number'].values\n",
    "        stores = df[['gpsx', 'gpsy']].values.tolist()\n",
    "        communities = map_data[['longitude', 'latitude']].values.tolist()\n",
    "\n",
    "        within_distance_meters = 410.0 # this is what we find in the RD design\n",
    "        # Perform network formation\n",
    "        network, edges = network_formulation.network_formation(stores, communities, effects, within_distance_meters)\n",
    "        \n",
    "        df_edges = pd.DataFrame(edges, columns=[\"store_id_1\", \"store_id_2\", \"effect\"])\n",
    "        \n",
    "        df_network = []\n",
    "        for x, comm_effects in enumerate(network):\n",
    "            for comm, effect in comm_effects:\n",
    "                df_network.append((x, comm, effect))\n",
    "        df_network = pd.DataFrame(df_network, columns=[\"store_id\", \"community_id\", \"effect\"])\n",
    "\n",
    "\n",
    "        # Calculate the average influential effect for each community\n",
    "        community_influence = calculate_influential_effects(df_network, df_edges)\n",
    "        \n",
    "        community_influence_df = pd.DataFrame(list(community_influence.items()), columns=['community', 'influence'])\n",
    "        community_influence_df['community_id'] = community_influence_df['community'].str.extract(r'(\\d+)').astype(int)\n",
    "        \n",
    "        map_data.reset_index(drop = True, inplace = True)\n",
    "        map_data['community_id'] = map_data.index\n",
    "        merged_data = pd.merge(map_data, community_influence_df, on='community_id', how='left')\n",
    "\n",
    "        # print(len(community_influence_df['community_id'].unique()))\n",
    "        # print(merged_data['influence'].isnull().sum())\n",
    "\n",
    "        merged_data['influence'] = merged_data['influence'].fillna(0)\n",
    "        \n",
    "        if i == 0 and j == 0:\n",
    "            combined_result = merged_data\n",
    "        else:\n",
    "            combined_result = pd.concat([combined_result, merged_data], ignore_index=True, verify_integrity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building_type        0\n",
      "village              0\n",
      "district             0\n",
      "floor_level          0\n",
      "new_lng              0\n",
      "                 ...  \n",
      "longitude            0\n",
      "latitude             0\n",
      "community_id         0\n",
      "community        74466\n",
      "influence            0\n",
      "Length: 117, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(combined_result.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217200\n"
     ]
    }
   ],
   "source": [
    "print(len(combined_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_result.to_csv('combined_result-with-network.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
