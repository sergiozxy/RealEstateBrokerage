{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import networkx as nx\n",
    "import pybind11\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import geopandas as gpd\n",
    "import shapely \n",
    "import os\n",
    "import sys\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "import random\n",
    "from geopy.distance import geodesic\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/xuyuan/Desktop/2024 summer/real estate paper/writing/RealEstateBrokerage/network_estimation')\n",
    "import network_formulation\n",
    "os.chdir('/home/xuyuan/Desktop/2024 summer/real estate paper/oritignal cleaning/RealEstateBrokerage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217200\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_stata('template.dta')\n",
    "codebook = {\n",
    "    1: '北京市',\n",
    "    2: '成都市',\n",
    "    3: '重庆市',\n",
    "    4: '广州市',\n",
    "    5: '杭州市',\n",
    "    6: '南京市',\n",
    "    7: '上海市',\n",
    "    8: '深圳市',\n",
    "    9: '天津市',\n",
    "    10: '武汉市'\n",
    "}\n",
    "\n",
    "data['city_id'] = data['city_id'].map(codebook)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "map_data = data[(data['city_id'] == '北京市') & (data['year'] == 2021)]\n",
    "print(len(map_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(df_network, df_edges):\n",
    "    # now we need to convert the problem using cpp\n",
    "    G = nx.Graph()\n",
    "\n",
    "    store_ids = set(df_network['store_id']).union(df_edges['store_id_1']).union(df_edges['store_id_2'])\n",
    "    for store_id in store_ids:\n",
    "        G.add_node(f'Store {store_id}', color='red', shape='o')\n",
    "\n",
    "    # Add nodes for communities\n",
    "    community_ids = set(df_network['community_id'])\n",
    "    for community_id in community_ids:\n",
    "        G.add_node(f'Community {community_id}', color='blue', shape='s')\n",
    "\n",
    "    # Add edges based on df_network\n",
    "    for _, row in df_network.iterrows():\n",
    "        store_id = row['store_id']\n",
    "        community_id = row['community_id']\n",
    "        effect = row['effect']\n",
    "        G.add_edge(f'Store {store_id}', f'Community {community_id}', weight=1, color='gray', style='dotted')\n",
    "        # we can also assign weight 1 for this result\n",
    "    # Add edges based on df_edges\n",
    "    for _, row in df_edges.iterrows():\n",
    "        store_id_1 = row['store_id_1']\n",
    "        store_id_2 = row['store_id_2']\n",
    "        effect = row['effect']\n",
    "        G.add_edge(f'Store {store_id_1}', f'Store {store_id_2}', weight=effect, color='black', style='solid')\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_summary_statistics(centrality_dict):\n",
    "    values = list(centrality_dict.values())\n",
    "    return {\n",
    "        'max': max(values),\n",
    "        'mean': np.mean(values),\n",
    "        'median': np.median(values)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now working on 北京市 in year 2016\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(0, len(data['city_id'].unique())):\n",
    "    map_data = data[data['city_id'] == data['city_id'].unique()[i]]\n",
    "    for j in range(0, len(map_data['year'].unique())):\n",
    "        print('now working on {} in year {}'.format(data['city_id'].unique()[i], data['year'].unique()[j]))\n",
    "        map_data = data[(data['city_id'] == data['city_id'].unique()[i]) & (data['year'] == data['year'].unique()[j])]\n",
    "        map_data = gpd.GeoDataFrame(map_data, geometry = map_data.geometry.apply(wkt.loads))\n",
    "        df = pd.read_csv(\"classifying brokerages/processed/{}_{}.csv\".format(data['year'].unique()[j] - 2000, data['city_id'].unique()[i]))\n",
    "        map_data['longitude'] = map_data['geometry'].apply(lambda point: point.x)\n",
    "        map_data['latitude'] = map_data['geometry'].apply(lambda point: point.y)\n",
    "        \n",
    "        effects = map_data['number'].values\n",
    "        stores = df[['gpsx', 'gpsy']].values.tolist()\n",
    "        communities = map_data[['longitude', 'latitude']].values.tolist()\n",
    "        \n",
    "        within_distance_meters = 410.0 # this is what we find in the RD design\n",
    "        # Perform network formation\n",
    "        network, edges = network_formulation.network_formation(stores, communities, effects, within_distance_meters)\n",
    "        \n",
    "        df_edges = pd.DataFrame(edges, columns=[\"store_id_1\", \"store_id_2\", \"effect\"])\n",
    "        df_network = []\n",
    "        for x, comm_effects in enumerate(network):\n",
    "            for comm, effect in comm_effects:\n",
    "                df_network.append((x, comm, effect))\n",
    "        df_network = pd.DataFrame(df_network, columns=[\"store_id\", \"community_id\", \"effect\"])\n",
    "        \n",
    "        G = construct_graph(df_network, df_edges)\n",
    "        \n",
    "        \n",
    "        local_clustering = nx.clustering(G, weight='weight')\n",
    "        average_clustering = nx.average_clustering(G, weight='weight')\n",
    "        global_clustering = nx.transitivity(G)\n",
    "        \n",
    "        degree_centrality = nx.degree_centrality(G)\n",
    "        betweenness_centrality = nx.betweenness_centrality(G, weight='weight')\n",
    "        closeness_centrality = nx.closeness_centrality(G, distance='weight')\n",
    "        \n",
    "        # Calculate summary statistics for degree centrality\n",
    "        degree_summary = calculate_summary_statistics(degree_centrality)\n",
    "        # Calculate summary statistics for betweenness centrality\n",
    "        betweenness_summary = calculate_summary_statistics(betweenness_centrality)\n",
    "        # Calculate summary statistics for closeness centrality\n",
    "        closeness_summary = calculate_summary_statistics(closeness_centrality)\n",
    "        \n",
    "        pagerank = nx.pagerank(G, weight='weight')\n",
    "        pagerank_summary = calculate_summary_statistics(pagerank)\n",
    "        \n",
    "        connectivity = nx.node_connectivity(G)\n",
    "        \n",
    "        result = {\n",
    "            'city_id': data['city_id'].unique()[i],\n",
    "            'year': data['year'].unique()[j],\n",
    "            'average_clustering': average_clustering,\n",
    "            'global_clustering': global_clustering,\n",
    "            'degree_centrality_max': degree_summary['max'],\n",
    "            'degree_centrality_mean': degree_summary['mean'],\n",
    "            'degree_centrality_median': degree_summary['median'],\n",
    "            'betweenness_centrality_max': betweenness_summary['max'],\n",
    "            'betweenness_centrality_mean': betweenness_summary['mean'],\n",
    "            'betweenness_centrality_median': betweenness_summary['median'],\n",
    "            'closeness_centrality_max': closeness_summary['max'],\n",
    "            'closeness_centrality_mean': closeness_summary['mean'],\n",
    "            'closeness_centrality_median': closeness_summary['median'],\n",
    "            'pagerank_max': pagerank_summary['max'],\n",
    "            'pagerank_mean': pagerank_summary['mean'],\n",
    "            'pagerank_median': pagerank_summary['median'],\n",
    "            'connectivity': connectivity\n",
    "        }\n",
    "        \n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to a CSV file if needed\n",
    "results_df.to_csv('network_measures_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
