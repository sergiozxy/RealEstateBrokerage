{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuyuan/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import networkx as nx\n",
    "import pybind11\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import geopandas as gpd\n",
    "import shapely \n",
    "import os\n",
    "import sys\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "import random\n",
    "from geopy.distance import geodesic\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/xuyuan/Desktop/2024 summer/real estate paper/writing/RealEstateBrokerage/network_estimation')\n",
    "import network_formulation\n",
    "os.chdir('/home/xuyuan/Desktop/2024 summer/real estate paper/oritignal cleaning/RealEstateBrokerage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217200\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_stata('template.dta')\n",
    "codebook = {\n",
    "    1: '北京市',\n",
    "    2: '成都市',\n",
    "    3: '重庆市',\n",
    "    4: '广州市',\n",
    "    5: '杭州市',\n",
    "    6: '南京市',\n",
    "    7: '上海市',\n",
    "    8: '深圳市',\n",
    "    9: '天津市',\n",
    "    10: '武汉市'\n",
    "}\n",
    "\n",
    "data['city_id'] = data['city_id'].map(codebook)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "map_data = data[(data['city_id'] == '北京市') & (data['year'] == 2021)]\n",
    "print(len(map_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph(df_network, df_edges):\n",
    "    # now we need to convert the problem using cpp\n",
    "    G = nx.Graph()\n",
    "\n",
    "    store_ids = set(df_network['store_id']).union(df_edges['store_id_1']).union(df_edges['store_id_2'])\n",
    "    for store_id in store_ids:\n",
    "        G.add_node(f'Store {store_id}', color='red', shape='o')\n",
    "\n",
    "    # Add nodes for communities\n",
    "    community_ids = set(df_network['community_id'])\n",
    "    for community_id in community_ids:\n",
    "        G.add_node(f'Community {community_id}', color='blue', shape='s')\n",
    "\n",
    "    # Add edges based on df_network\n",
    "    for _, row in df_network.iterrows():\n",
    "        store_id = row['store_id']\n",
    "        community_id = row['community_id']\n",
    "        effect = row['effect']\n",
    "        G.add_edge(f'Store {store_id}', f'Community {community_id}', weight=effect, color='gray', style='dotted')\n",
    "\n",
    "    # Add edges based on df_edges\n",
    "    for _, row in df_edges.iterrows():\n",
    "        store_id_1 = row['store_id_1']\n",
    "        store_id_2 = row['store_id_2']\n",
    "        effect = row['effect']\n",
    "        G.add_edge(f'Store {store_id_1}', f'Store {store_id_2}', weight=effect, color='black', style='solid')\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_summary_statistics(centrality_dict):\n",
    "    values = list(centrality_dict.values())\n",
    "    return {\n",
    "        'max': max(values),\n",
    "        'mean': np.mean(values),\n",
    "        'median': np.median(values)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now working on 北京市 in year 2016\n",
      "now working on 北京市 in year 2017\n",
      "now working on 北京市 in year 2018\n",
      "now working on 北京市 in year 2019\n",
      "now working on 北京市 in year 2020\n",
      "now working on 北京市 in year 2021\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m G \u001b[38;5;241m=\u001b[39m construct_graph(df_network, df_edges)\n\u001b[1;32m     30\u001b[0m local_clustering \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mclustering(G)\n\u001b[0;32m---> 31\u001b[0m average_clustering \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage_clustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m global_clustering \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mtransitivity(G)\n\u001b[1;32m     34\u001b[0m degree_centrality \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mdegree_centrality(G)\n",
      "File \u001b[0;32m<class 'networkx.utils.decorators.argmap'> compilation 12:3\u001b[0m, in \u001b[0;36margmap_average_clustering_9\u001b[0;34m(G, nodes, weight, count_zeros, backend, **backend_kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbz2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/networkx/utils/backends.py:633\u001b[0m, in \u001b[0;36m_dispatchable.__call__\u001b[0;34m(self, backend, *args, **kwargs)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the result of the original function, or the backend function if\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03mthe backend is specified and that backend implements `func`.\"\"\"\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m backends:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# Fast path if no backends are installed\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`\u001b[39;00m\n\u001b[1;32m    636\u001b[0m backend_name \u001b[38;5;241m=\u001b[39m backend\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/networkx/algorithms/cluster.py:307\u001b[0m, in \u001b[0;36maverage_clustering\u001b[0;34m(G, nodes, weight, count_zeros)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m count_zeros:\n\u001b[1;32m    306\u001b[0m     c \u001b[38;5;241m=\u001b[39m [v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m c \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(v) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(0, len(data['city_id'].unique())):\n",
    "    map_data = data[data['city_id'] == data['city_id'].unique()[i]]\n",
    "    for j in range(0, len(map_data['year'].unique())):\n",
    "        print('now working on {} in year {}'.format(data['city_id'].unique()[i], data['year'].unique()[j]))\n",
    "        map_data = data[(data['city_id'] == data['city_id'].unique()[i]) & (data['year'] == data['year'].unique()[j])]\n",
    "        map_data = gpd.GeoDataFrame(map_data, geometry = map_data.geometry.apply(wkt.loads))\n",
    "        df = pd.read_csv(\"classifying brokerages/processed/{}_{}.csv\".format(data['year'].unique()[j] - 2000, data['city_id'].unique()[i]))\n",
    "        map_data['longitude'] = map_data['geometry'].apply(lambda point: point.x)\n",
    "        map_data['latitude'] = map_data['geometry'].apply(lambda point: point.y)\n",
    "        \n",
    "        effects = map_data['number'].values\n",
    "        stores = df[['gpsx', 'gpsy']].values.tolist()\n",
    "        communities = map_data[['longitude', 'latitude']].values.tolist()\n",
    "        \n",
    "        within_distance_meters = 410.0 # this is what we find in the RD design\n",
    "        # Perform network formation\n",
    "        network, edges = network_formulation.network_formation(stores, communities, effects, within_distance_meters)\n",
    "        \n",
    "        df_edges = pd.DataFrame(edges, columns=[\"store_id_1\", \"store_id_2\", \"effect\"])\n",
    "        df_network = []\n",
    "        for x, comm_effects in enumerate(network):\n",
    "            for comm, effect in comm_effects:\n",
    "                df_network.append((x, comm, effect))\n",
    "        df_network = pd.DataFrame(df_network, columns=[\"store_id\", \"community_id\", \"effect\"])\n",
    "        \n",
    "        G = construct_graph(df_network, df_edges)\n",
    "        \n",
    "        \n",
    "        local_clustering = nx.clustering(G)\n",
    "        average_clustering = nx.average_clustering(G)\n",
    "        global_clustering = nx.transitivity(G)\n",
    "        \n",
    "        degree_centrality = nx.degree_centrality(G)\n",
    "        betweenness_centrality = nx.betweenness_centrality(G)\n",
    "        closeness_centrality = nx.closeness_centrality(G)\n",
    "        \n",
    "        # Calculate summary statistics for degree centrality\n",
    "        degree_summary = calculate_summary_statistics(degree_centrality)\n",
    "        # Calculate summary statistics for betweenness centrality\n",
    "        betweenness_summary = calculate_summary_statistics(betweenness_centrality)\n",
    "        # Calculate summary statistics for closeness centrality\n",
    "        closeness_summary = calculate_summary_statistics(closeness_centrality)\n",
    "        \n",
    "        pagerank = nx.pagerank(G)\n",
    "        pagerank_summary = calculate_summary_statistics(pagerank)\n",
    "        \n",
    "        connectivity = nx.node_connectivity(G)\n",
    "        \n",
    "        result = {\n",
    "            'city_id': data['city_id'].unique()[i],\n",
    "            'year': data['year'].unique()[j],\n",
    "            'average_clustering': average_clustering,\n",
    "            'global_clustering': global_clustering,\n",
    "            'degree_centrality_max': degree_summary['max'],\n",
    "            'degree_centrality_mean': degree_summary['mean'],\n",
    "            'degree_centrality_median': degree_summary['median'],\n",
    "            'betweenness_centrality_max': betweenness_summary['max'],\n",
    "            'betweenness_centrality_mean': betweenness_summary['mean'],\n",
    "            'betweenness_centrality_median': betweenness_summary['median'],\n",
    "            'closeness_centrality_max': closeness_summary['max'],\n",
    "            'closeness_centrality_mean': closeness_summary['mean'],\n",
    "            'closeness_centrality_median': closeness_summary['median'],\n",
    "            'pagerank_max': pagerank_summary['max'],\n",
    "            'pagerank_mean': pagerank_summary['mean'],\n",
    "            'pagerank_median': pagerank_summary['median'],\n",
    "            'connectivity': connectivity\n",
    "        }\n",
    "        \n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save to a CSV file if needed\n",
    "results_df.to_csv('network_measures_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
